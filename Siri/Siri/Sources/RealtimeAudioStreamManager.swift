import Foundation
import AVFoundation
import Speech
import os.log

@MainActor
public class RealtimeAudioStreamManager: NSObject, ObservableObject {
    
    @Published public var isProcessing: Bool = false
    @Published public var recognizedText: String = ""
    @Published public var errorMessage: String = ""
    
    // MARK: - Private Properties for text management
    private var previousText: String = ""
    private var shouldPreserveText: Bool = false
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let logger = Logger(subsystem: "dev.tuist.Siri", category: "RealtimeAudio")
    private let appGroupID = "group.dev.tuist.Siri"
    
    private var darwinNotificationCenter: CFNotificationCenter?
    private var audioBufferQueue = [CMSampleBuffer]()
    
    // ç”¨äºä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®è¿›è¡ŒéªŒè¯
    private var audioWriter: AVAssetWriter?
    private var audioWriterInput: AVAssetWriterInput?
    private var convertedAudioFileURL: URL?
    private var processingQueue = DispatchQueue(label: "realtime.audio.processing", qos: .userInitiated)
    private var hasLoggedFormat = false
    
    // ä¿å­˜éŸ³é¢‘åˆ°m4aæ–‡ä»¶ - å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„æ–¹å¼
    private var m4aAudioWriter: AVAssetWriter?
    private var m4aAudioWriterInput: AVAssetWriterInput?
    private var currentM4AFileURL: URL?
    private var m4aStartTime: CMTime?
    
    public override init() {
        super.init()
        // è®¾ç½®éŸ³é¢‘ä¼šè¯ç¡®ä¿æ‰¬å£°å™¨è¾“å‡º
        setupAudioSession()
        speechRecognizer?.delegate = self
        setupDarwinNotifications()
    }
    
    public func startMonitoring() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            errorMessage = "è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨"
            return
        }
        
        SFSpeechRecognizer.requestAuthorization { [weak self] authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self?.startRecognition()
                case .denied:
                    self?.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»"
                case .restricted:
                    self?.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™å—é™"
                case .notDetermined:
                    self?.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™æœªç¡®å®š"
                @unknown default:
                    self?.errorMessage = "æœªçŸ¥çš„æƒé™çŠ¶æ€"
                }
            }
        }
    }
    
    public func stopMonitoring() {
        logger.info("ğŸ›‘ åœæ­¢å®æ—¶éŸ³é¢‘æµç›‘æ§")
        stopRecognition()
    }
    
    // MARK: - Text Preservation Methods
    private var textPreservationRequested: Bool = false  // è·Ÿè¸ªæ˜¯å¦è¯·æ±‚äº†æ–‡å­—ä¿ç•™
    
    public func setTextPreservationMode(_ preserve: Bool) {
        textPreservationRequested = preserve
        shouldPreserveText = preserve
        if preserve {
            // ä¿å­˜å½“å‰æ–‡å­—
            previousText = recognizedText
            logger.info("ğŸ”’ å¯ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œä¿å­˜æ–‡å­—: '\(self.previousText)'")
        } else {
            // æ¸…é™¤ä¿å­˜çš„æ–‡å­—
            previousText = ""
            textPreservationRequested = false
            logger.info("ğŸ”“ ç¦ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼")
        }
    }
    
    private func setupDarwinNotifications() {
        darwinNotificationCenter = CFNotificationCenterGetDarwinNotifyCenter()
        
        let notificationName = "dev.tuist.Siri.audiodata" as CFString
        let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque())
        
        CFNotificationCenterAddObserver(
            darwinNotificationCenter,
            observer,
            { (center, observer, name, object, userInfo) in
                guard let observer = observer else { return }
                let manager = Unmanaged<RealtimeAudioStreamManager>.fromOpaque(observer).takeUnretainedValue()
                Task { @MainActor in
                    manager.handleAudioDataNotification()
                }
            },
            notificationName,
            nil,
            .deliverImmediately
        )
        
        logger.info("ğŸ“¡ Darwiné€šçŸ¥ç›‘å¬å·²è®¾ç½®")
    }
    
    private func handleAudioDataNotification() {
        logger.debug("ğŸ“± æ”¶åˆ°éŸ³é¢‘æ•°æ®é€šçŸ¥")
        readAudioDataFromSharedMemory()
    }
    
    private func readAudioDataFromSharedMemory() {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let audioDataURL = containerURL.appendingPathComponent("realtime_audio_buffer.data")
        let formatURL = containerURL.appendingPathComponent("realtime_audio_format.json")
        
        guard FileManager.default.fileExists(atPath: audioDataURL.path),
              FileManager.default.fileExists(atPath: formatURL.path),
              let audioData = try? Data(contentsOf: audioDataURL),
              let formatData = try? Data(contentsOf: formatURL),
              let formatInfo = try? JSONSerialization.jsonObject(with: formatData) as? [String: Any] else {
            return
        }
        
        processingQueue.async { [weak self] in
            Task { @MainActor in
                self?.processAudioData(audioData, formatInfo: formatInfo)
            }
        }
    }
    
    private func processAudioData(_ data: Data, formatInfo: [String: Any]) {
        // é¦–å…ˆï¼Œä½¿ç”¨åŸå§‹æ•°æ®é‡å»ºCMSampleBufferå¹¶ä¿å­˜åˆ°m4aæ–‡ä»¶ï¼ˆå®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerï¼‰
        if let sampleBuffer = createSampleBufferFromData(data, formatInfo: formatInfo) {
            saveOriginalAudioToFile(sampleBuffer)
            
            // ä½¿ç”¨é‡å»ºçš„CMSampleBufferè¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼ˆæ•°æ®æºå·²éªŒè¯æ­£å¸¸ï¼‰
            if isProcessing, recognitionRequest != nil {
                performSpeechRecognitionWithSampleBuffer(sampleBuffer)
                return
            }
        }
        
        guard isProcessing,
              let recognitionRequest = recognitionRequest else {
            return
        }
        
        // ä»æ ¼å¼ä¿¡æ¯ä¸­è·å–éŸ³é¢‘å‚æ•°
        guard let sampleRate = formatInfo["sampleRate"] as? Double,
              let channels = formatInfo["channels"] as? UInt32,
              let formatID = formatInfo["formatID"] as? UInt32,
              let bitsPerChannel = formatInfo["bitsPerChannel"] as? UInt32 else {
            logger.error("âŒ æ— æ³•è§£æéŸ³é¢‘æ ¼å¼ä¿¡æ¯")
            return
        }
        
        // åªåœ¨é¦–æ¬¡è¯†åˆ«æ ¼å¼æ—¶è®°å½•
        if !hasLoggedFormat && (formatID == kAudioFormatLinearPCM || formatID == 1819304813) {
            logger.info("ğŸµ PCMæ ¼å¼: \(sampleRate)Hz, \(channels)å£°é“, \(bitsPerChannel)ä½")
            hasLoggedFormat = true
        }
        
        // åˆ›å»ºåˆé€‚çš„éŸ³é¢‘æ ¼å¼
        var audioFormat: AVAudioFormat?
        
        // æ ¹æ®å®é™…æ ¼å¼åˆ›å»ºAVAudioFormat - ä½¿ç”¨ä¸æˆåŠŸWAVè½¬æ¢ç›¸åŒçš„æ ¼å¼
        // kAudioFormatLinearPCM = 1819304813 ('lpcm') æ˜¯çº¿æ€§PCMæ ¼å¼çš„æ ‡è¯†ç¬¦
        if formatID == kAudioFormatLinearPCM || formatID == 1819304813 {
            // PCMæ ¼å¼ - å‚è€ƒæˆåŠŸçš„WAVè½¬æ¢æ ¼å¼
            if bitsPerChannel == 16 {
                // 16ä½æ•´æ•° - ä½¿ç”¨ä¸WAVè½¬æ¢ç›¸åŒçš„æ ¼å¼å‚æ•°
                // å‚è€ƒAudioFileManagerä¸­æˆåŠŸçš„è½¬æ¢æ ¼å¼ï¼š
                // mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagIsPacked | kAudioFormatFlagsNativeEndian
                var asbd = AudioStreamBasicDescription()
                asbd.mSampleRate = sampleRate
                asbd.mFormatID = kAudioFormatLinearPCM
                asbd.mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagIsPacked | kAudioFormatFlagsNativeEndian
                asbd.mBitsPerChannel = 16
                asbd.mChannelsPerFrame = UInt32(channels)
                asbd.mBytesPerFrame = asbd.mChannelsPerFrame * 2
                asbd.mFramesPerPacket = 1
                asbd.mBytesPerPacket = asbd.mBytesPerFrame
                
                audioFormat = AVAudioFormat(streamDescription: &asbd)
                logger.info("ğŸµ ä½¿ç”¨WAVå…¼å®¹æ ¼å¼: 16-bit signed integer, native endian, \(channels)å£°é“")
            } else if bitsPerChannel == 32 {
                // 32ä½æµ®ç‚¹ - ä¿æŒåŸæœ‰æ ¼å¼
                audioFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: AVAudioChannelCount(channels), interleaved: false)
                logger.info("ğŸµ ä½¿ç”¨32ä½æµ®ç‚¹æ ¼å¼")
            }
        }
        
        guard let format = audioFormat else {
            logger.error("âŒ ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: formatID=\(formatID), bitsPerChannel=\(bitsPerChannel)")
            return
        }
        
        // è®¡ç®—å¸§æ•°é‡
        let bytesPerSample = bitsPerChannel / 8
        let frameCount = AVAudioFrameCount(data.count / (Int(bytesPerSample) * Int(channels)))
        
        guard frameCount > 0 else {
            return
        }
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
            return
        }
        
        audioBuffer.frameLength = frameCount
        
        // å¤åˆ¶éŸ³é¢‘æ•°æ® - 16ä½æ ¼å¼ä½¿ç”¨äº¤é”™æ•°æ®ï¼ˆä¸WAVè½¬æ¢æ ¼å¼ä¸€è‡´ï¼‰
        data.withUnsafeBytes { (rawBytes: UnsafeRawBufferPointer) in
            if bitsPerChannel == 16 {
                // 16ä½æ•´æ•°æ•°æ® - ä½¿ç”¨äº¤é”™æ ¼å¼ï¼ˆä¸æˆåŠŸçš„WAVè½¬æ¢ä¸€è‡´ï¼‰
                guard let int16Pointer = rawBytes.bindMemory(to: Int16.self).baseAddress else {
                    return
                }
                
                // å¯¹äºäº¤é”™æ ¼å¼ï¼Œç›´æ¥å¤åˆ¶åŸå§‹æ•°æ®
                if let audioDataPointer = audioBuffer.audioBufferList.pointee.mBuffers.mData {
                    let sampleCount = Int(frameCount) * Int(channels)
                    let audioInt16Pointer = audioDataPointer.bindMemory(to: Int16.self, capacity: sampleCount)
                    audioInt16Pointer.initialize(from: int16Pointer, count: sampleCount)
                    
                    // éªŒè¯æ•°æ®
                    let firstSample = int16Pointer[0]
                    let secondSample = channels > 1 ? int16Pointer[1] : firstSample
                    logger.info("ğŸ” äº¤é”™æ ¼å¼å¤åˆ¶: é¦–æ ·æœ¬=\(firstSample), æ¬¡æ ·æœ¬=\(secondSample), æ€»æ ·æœ¬æ•°=\(sampleCount)")
                }
                
            } else if bitsPerChannel == 32 {
                // 32ä½æµ®ç‚¹æ•°æ® - ä»äº¤é”™è½¬ä¸ºéäº¤é”™
                guard let floatPointer = rawBytes.bindMemory(to: Float.self).baseAddress,
                      let channelData = audioBuffer.floatChannelData else {
                    return
                }
                
                if channels == 2 {
                    // ç«‹ä½“å£°ï¼šåˆ†ç¦»äº¤é”™æ•°æ®
                    let leftChannel = channelData[0]
                    let rightChannel = channelData[1]
                    
                    for frame in 0..<Int(frameCount) {
                        let interleavedIndex = frame * 2
                        leftChannel[frame] = floatPointer[interleavedIndex]
                        rightChannel[frame] = floatPointer[interleavedIndex + 1]
                    }
                } else {
                    // å•å£°é“ï¼šç›´æ¥å¤åˆ¶
                    let channel = channelData[0]
                    channel.initialize(from: floatPointer, count: Int(frameCount))
                }
            }
        }
        
        // æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥ä¿å­˜audioBufferï¼Œå› ä¸ºå®ƒæ˜¯è½¬æ¢åçš„æ ¼å¼
        // æˆ‘ä»¬éœ€è¦ä¿å­˜åŸå§‹çš„CMSampleBufferï¼Œä½†è¿™é‡Œåªæœ‰è½¬æ¢åçš„AVAudioPCMBuffer
        // æ‰€ä»¥m4aä¿å­˜éœ€è¦åœ¨processAudioDataä¸­è¿›è¡Œï¼Œä½¿ç”¨åŸå§‹æ•°æ®
        
        // ä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®ç”¨äºéªŒè¯
        saveConvertedAudioBuffer(audioBuffer)  // é‡æ–°å¯ç”¨ï¼Œæµ‹è¯•æ–°çš„äº¤é”™æ ¼å¼
        
        // å‘é€åˆ°è¯­éŸ³è¯†åˆ«å™¨
        recognitionRequest.append(audioBuffer)
    }
    
    private func calculateAudioLevel(from audioBuffer: AVAudioPCMBuffer) -> Double {
        // å¯¹äºäº¤é”™æ ¼å¼ï¼Œä½¿ç”¨audioBufferListè®¿é—®æ•°æ®
        guard let audioDataPointer = audioBuffer.audioBufferList.pointee.mBuffers.mData else {
            return 0.0
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(audioBuffer.format.channelCount)
        let sampleCount = frameCount * channels
        
        let int16Pointer = audioDataPointer.bindMemory(to: Int16.self, capacity: sampleCount)
        
        var sum: Double = 0.0
        
        for i in 0..<sampleCount {
            let sample = Double(int16Pointer[i]) / 32768.0 // å½’ä¸€åŒ–åˆ° -1.0 åˆ° 1.0
            sum += sample * sample
        }
        
        let rms = sqrt(sum / Double(sampleCount))
        return rms
    }
    
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            // ä½¿ç”¨ playback æ¨¡å¼ï¼Œç¡®ä¿ä»æ‰¬å£°å™¨è¾“å‡ºï¼ŒåŒæ—¶æ”¯æŒä¸å…¶ä»–éŸ³é¢‘æ··åˆ
            // ç§»é™¤ .duckOthers é€‰é¡¹ï¼Œé¿å…å¹²æ‰°å…¶ä»–éŸ³é¢‘æ’­æ”¾
            try audioSession.setCategory(.playback, mode: .default, options: [.mixWithOthers])
            // å¼ºåˆ¶è®¾ç½®éŸ³é¢‘è·¯ç”±åˆ°æ‰¬å£°å™¨
            try audioSession.overrideOutputAudioPort(.speaker)
            logger.info("ğŸµ éŸ³é¢‘ä¼šè¯è®¾ç½®æˆåŠŸ (playback + default)")
        } catch {
            logger.error("âŒ éŸ³é¢‘ä¼šè¯è®¾ç½®å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func startRecognition() {
        logger.info("ğŸš€ å¼€å§‹è¯­éŸ³è¯†åˆ«")
        
        guard !isProcessing else {
            return
        }
        
        // å¦‚æœä¹‹å‰è¯·æ±‚äº†æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œé‡æ–°å¯ç”¨
        if textPreservationRequested {
            shouldPreserveText = true
            logger.info("ğŸ”„ æ¢å¤æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œä¹‹å‰ä¿å­˜çš„æ–‡å­—: '\(self.previousText)'")
        }
        
        // ç¡®ä¿éŸ³é¢‘ä»æ‰¬å£°å™¨è¾“å‡º
        do {
            try AVAudioSession.sharedInstance().overrideOutputAudioPort(.speaker)
            logger.info("ğŸ”Š å¼ºåˆ¶éŸ³é¢‘è·¯ç”±åˆ°æ‰¬å£°å™¨")
        } catch {
            logger.error("âŒ è®¾ç½®æ‰¬å£°å™¨è¾“å‡ºå¤±è´¥: \(error.localizedDescription)")
        }
        
        isProcessing = true
        hasLoggedFormat = false
        
        // é‡ç½®m4aå½•åˆ¶çŠ¶æ€
        m4aStartTime = nil
        
        // å¼€å§‹m4aæ–‡ä»¶å½•åˆ¶ - å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandler
        startM4ARecording()
        
        // å¼€å§‹å½•åˆ¶è½¬æ¢åçš„éŸ³é¢‘ç”¨äºéªŒè¯ - åªä¿å­˜WAVç”¨äºéªŒè¯éŸ³é¢‘è´¨é‡
        startConvertedAudioRecording()  // ä¿å­˜ç”¨äºè¯­éŸ³è¯†åˆ«çš„éŸ³é¢‘æ•°æ®ä¸ºWAVæ ¼å¼
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            errorMessage = "æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚"
            isProcessing = false
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            DispatchQueue.main.async {
                if let result = result {
                    let newText = result.bestTranscription.formattedString
                    let isFinal = result.isFinal
                    
                    // è¯¦ç»†æ—¥å¿—
                    self?.logger.info("ğŸ¤ è¯­éŸ³è¯†åˆ«ç»“æœ: '\(newText)' (æœ€ç»ˆç»“æœ: \(isFinal))")
                    
                    if !newText.isEmpty {
                        if self?.shouldPreserveText == true, let previousText = self?.previousText {
                            // è¿½åŠ æ¨¡å¼ï¼šä¿ç•™ä¹‹å‰çš„æ–‡å­—ï¼Œæ·»åŠ æ–°å†…å®¹
                            if !previousText.isEmpty {
                                self?.recognizedText = previousText + "\n" + newText
                                self?.logger.info("ğŸ¯ è¯†åˆ«æ–‡æœ¬è¿½åŠ : '\(previousText)' + '\(newText)'")
                            } else {
                                self?.recognizedText = newText
                                self?.logger.info("ğŸ¯ è¯†åˆ«æ–‡æœ¬æ›´æ–°: \(newText)")
                            }
                        } else {
                            // æ›¿æ¢æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æ–°æ–‡å­—
                            self?.recognizedText = newText
                            self?.logger.info("ğŸ¯ è¯†åˆ«æ–‡æœ¬æ›´æ–°: \(newText)")
                        }
                    } else {
                        self?.logger.info("âš ï¸ è¯†åˆ«ç»“æœä¸ºç©ºæ–‡æœ¬")
                    }
                } else {
                    self?.logger.info("âš ï¸ è¯†åˆ«ç»“æœä¸º nil")
                }
                
                if let error = error {
                    self?.logger.error("âŒ è¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
                    self?.logger.error("âŒ é”™è¯¯è¯¦ç»†ä¿¡æ¯: \(error)")
                    
                    // æ£€æŸ¥æ˜¯å¦æ˜¯"No speech detected"é”™è¯¯
                    if error.localizedDescription.contains("No speech") || error.localizedDescription.contains("no speech") {
                        self?.logger.info("âš ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³ - å¯èƒ½éŸ³é¢‘å†…å®¹ä¸ºé™éŸ³æˆ–éŸ³é‡è¿‡ä½")
                    }
                    
                    // ä¸è¦å› ä¸ºè¯†åˆ«é”™è¯¯å°±åœæ­¢æ•´ä¸ªè¯†åˆ«è¿‡ç¨‹ï¼Œè¿™æ ·å¯ä»¥ç»§ç»­æ¥æ”¶éŸ³é¢‘
                    self?.errorMessage = error.localizedDescription
                } else {
                    // æ— é”™è¯¯æ—¶æ¸…ç©ºé”™è¯¯æ¶ˆæ¯
                    if self?.errorMessage != "" {
                        self?.errorMessage = ""
                    }
                }
            }
        }
        
        logger.info("âœ… è¯­éŸ³è¯†åˆ«ä»»åŠ¡å·²å¯åŠ¨")
    }
    
    private func stopRecognition() {
        logger.info("ğŸ›‘ åœæ­¢è¯­éŸ³è¯†åˆ«")
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        isProcessing = false
        
        // åœæ­¢è¯†åˆ«æ—¶æš‚æ—¶ç¦ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œé˜²æ­¢å»¶è¿Ÿå›è°ƒå¯¼è‡´æ–‡å­—é‡å¤
        if shouldPreserveText {
            logger.info("â¸ï¸ åœæ­¢è¯†åˆ«æ—¶æš‚æ—¶ç¦ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼")
            shouldPreserveText = false
        }
        
        // åœæ­¢m4aæ–‡ä»¶å½•åˆ¶ - æ¨¡ä»¿ScreenBroadcastHandlerçš„stopAudioRecording
        stopM4ARecording()
        
        // åœæ­¢å½•åˆ¶è½¬æ¢åçš„éŸ³é¢‘
        // stopConvertedAudioRecording()  // æš‚æ—¶æ³¨é‡Šæ‰ï¼Œåªæµ‹è¯•M4Aåˆ°WAVè½¬æ¢
        
        logger.info("âœ… è¯­éŸ³è¯†åˆ«å·²åœæ­¢")
    }
    
    deinit {
        let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque())
        CFNotificationCenterRemoveObserver(
            darwinNotificationCenter,
            observer,
            CFNotificationName("dev.tuist.Siri.audiodata" as CFString),
            nil
        )
    }
    
    // MARK: - Converted Audio Recording for Verification
    
    private func startConvertedAudioRecording() {
        // ç®€åŒ–ï¼šä¸å†åˆ›å»ºç‹¬ç«‹çš„éŸ³é¢‘æ–‡ä»¶ï¼Œè€Œæ˜¯åœ¨ç»“æŸæ—¶ä½¿ç”¨M4Aæ–‡ä»¶è½¬æ¢
        logger.info("ğŸ™ï¸ å¼€å§‹è¯­éŸ³è¯†åˆ«ä¼šè¯ - å°†åœ¨å½•åˆ¶ç»“æŸæ—¶ä»M4Aæ–‡ä»¶åˆ›å»ºéªŒè¯WAV")
    }
    
    private func saveConvertedAudioBuffer(_ audioBuffer: AVAudioPCMBuffer) {
        // ç®€åŒ–ï¼šä¸å†ä¿å­˜éŸ³é¢‘æ•°æ®ï¼Œä¸“æ³¨äºè¯­éŸ³è¯†åˆ«åŠŸèƒ½
        // éªŒè¯éŸ³é¢‘è´¨é‡çš„WAVæ–‡ä»¶å°†åœ¨å½•åˆ¶ç»“æŸæ—¶ä»M4Aæ–‡ä»¶ç”Ÿæˆ
        
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(audioBuffer.format.channelCount)
        logger.debug("ğŸ”„ å¤„ç†éŸ³é¢‘ç¼“å†²åŒº: \(frameCount)å¸§, \(channels)å£°é“")
    }
    
    private func stopConvertedAudioRecording() {
        // ä½¿ç”¨æœ€æ–°çš„M4Aæ–‡ä»¶è½¬æ¢ä¸ºWAVç”¨äºéªŒè¯è¯­éŸ³è¯†åˆ«éŸ³é¢‘è´¨é‡
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        
        // æŸ¥æ‰¾æœ€æ–°çš„M4Aæ–‡ä»¶ï¼ˆä¸M4Aè½¬WAVè½¬æ¢ä½¿ç”¨ç›¸åŒæ•°æ®æºï¼‰
        do {
            let files = try FileManager.default.contentsOfDirectory(at: audioDirectory, includingPropertiesForKeys: [.creationDateKey])
            
            let m4aFiles = files.filter { $0.pathExtension == "m4a" && $0.lastPathComponent.hasPrefix("SystemAudio_") }
                .sorted { file1, file2 in
                    let date1 = (try? file1.resourceValues(forKeys: [.creationDateKey]).creationDate) ?? Date.distantPast
                    let date2 = (try? file2.resourceValues(forKeys: [.creationDateKey]).creationDate) ?? Date.distantPast
                    return date1 > date2
                }
            
            guard let latestM4A = m4aFiles.first else {
                logger.info("âš ï¸ æ²¡æœ‰æ‰¾åˆ°M4Aæ–‡ä»¶ç”¨äºéªŒè¯")
                return
            }
            
            // åˆ›å»ºéªŒè¯WAVæ–‡ä»¶å
            let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium).replacingOccurrences(of: ":", with: "-")
            let fileName = "RealtimeRecognition_\(timestamp).wav"
            let finalURL = audioDirectory.appendingPathComponent(fileName)
            
            // ä½¿ç”¨ä¸æ­£å¸¸è½¬æ¢ç›¸åŒçš„æ–¹æ³•è½¬æ¢M4Aåˆ°WAV
            let audioFileManager = AudioFileManager()
            if let wavURL = audioFileManager.convertM4AToWAV(m4aURL: latestM4A) {
                // é‡å‘½åä¸ºéªŒè¯æ–‡ä»¶
                try FileManager.default.moveItem(at: wavURL, to: finalURL)
                logger.info("âœ… è¯­éŸ³è¯†åˆ«éªŒè¯WAVæ–‡ä»¶å·²åˆ›å»º: \(fileName)")
                notifyConvertedAudioFileCompleted(fileURL: finalURL)
            } else {
                logger.error("âŒ M4Aè½¬WAVå¤±è´¥")
            }
            
        } catch {
            logger.error("âŒ æŸ¥æ‰¾M4Aæ–‡ä»¶å¤±è´¥: \(error.localizedDescription)")
        }
        
        // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        let tempDataURL = audioDirectory.appendingPathComponent("realtime_recognition_temp.pcm")
        try? FileManager.default.removeItem(at: tempDataURL)
        
        audioWriter = nil
        audioWriterInput = nil
        convertedAudioFileURL = nil
    }
    
    private func convertPCMToWAV(inputURL: URL, outputURL: URL) {
        do {
            let pcmData = try Data(contentsOf: inputURL)
            guard pcmData.count > 0 else {
                logger.error("âŒ PCMæ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºWAVæ–‡ä»¶")
                return
            }
            
            let wavData = createWAVHeader(dataLength: pcmData.count) + pcmData
            try wavData.write(to: outputURL)
            logger.info("ğŸ“ WAVæ–‡ä»¶å·²åˆ›å»º: \(outputURL.lastPathComponent), å¤§å°: \(wavData.count) bytes (PCM: \(pcmData.count) bytes)")
        } catch {
            logger.error("âŒ è½¬æ¢PCMä¸ºWAVå¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func createWAVHeader(dataLength: Int) -> Data {
        var header = Data()
        
        // RIFF header (12 bytes)
        header.append("RIFF".data(using: .ascii)!)                                    // "RIFF" (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(36 + dataLength).littleEndian) { Data($0) }) // File size - 8 (4 bytes)
        header.append("WAVE".data(using: .ascii)!)                                    // "WAVE" (4 bytes)
        
        // fmt sub-chunk (24 bytes)
        header.append("fmt ".data(using: .ascii)!)                                    // "fmt " (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(16).littleEndian) { Data($0) })      // fmt chunk size (4 bytes)
        header.append(withUnsafeBytes(of: UInt16(1).littleEndian) { Data($0) })       // PCM = 1 (2 bytes)
        header.append(withUnsafeBytes(of: UInt16(2).littleEndian) { Data($0) })       // 2 channels (2 bytes)
        header.append(withUnsafeBytes(of: UInt32(44100).littleEndian) { Data($0) })   // Sample rate (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(44100 * 2 * 2).littleEndian) { Data($0) }) // Byte rate (4 bytes)
        header.append(withUnsafeBytes(of: UInt16(4).littleEndian) { Data($0) })       // Block align = channels * bits/8 (2 bytes)
        header.append(withUnsafeBytes(of: UInt16(16).littleEndian) { Data($0) })      // Bits per sample (2 bytes)
        
        // data sub-chunk (8 bytes header + data)
        header.append("data".data(using: .ascii)!)                                    // "data" (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(dataLength).littleEndian) { Data($0) }) // Data length (4 bytes)
        
        logger.info("ğŸ“‹ WAVå¤´åˆ›å»º: æ€»å¤§å°=\(header.count + dataLength), å¤´=\(header.count)å­—èŠ‚, PCM=\(dataLength)å­—èŠ‚")
        
        return header
    }
    
    private func notifyConvertedAudioFileCompleted(fileURL: URL) {
        let notification: [String: Any] = [
            "event": "converted_audio_file_completed",
            "fileName": fileURL.lastPathComponent,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("converted_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            logger.info("ğŸ“¡ å·²é€šçŸ¥è½¬æ¢åéŸ³é¢‘æ–‡ä»¶å®Œæˆ")
        } catch {
            logger.error("âŒ å†™å…¥è½¬æ¢åéŸ³é¢‘é€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    // MARK: - M4A Audio Recording
    
    // å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„startAudioRecordingæ–¹æ³•
    private func startM4ARecording() {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            logger.error("âŒ æ— æ³•è·å–App Groupå®¹å™¨è·¯å¾„")
            return
        }
        
        // åˆ›å»ºéŸ³é¢‘ç›®å½•
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        if !FileManager.default.fileExists(atPath: audioDirectory.path) {
            do {
                try FileManager.default.createDirectory(at: audioDirectory, withIntermediateDirectories: true, attributes: nil)
            } catch {
                logger.error("âŒ åˆ›å»ºéŸ³é¢‘ç›®å½•å¤±è´¥: \(error.localizedDescription)")
                return
            }
        }
        
        // åˆ›å»ºéŸ³é¢‘æ–‡ä»¶
        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium).replacingOccurrences(of: ":", with: "-")
        let fileName = "RealtimeRecognition_\(timestamp).m4a"
        currentM4AFileURL = audioDirectory.appendingPathComponent(fileName)
        
        guard let audioFileURL = currentM4AFileURL else { return }
        
        // è®¾ç½®éŸ³é¢‘å†™å…¥å™¨ - å®Œå…¨å¤åˆ¶ScreenBroadcastHandlerçš„é…ç½®
        do {
            m4aAudioWriter = try AVAssetWriter(outputURL: audioFileURL, fileType: .m4a)
            
            // é…ç½®éŸ³é¢‘è®¾ç½® - ä¸ScreenBroadcastHandlerå®Œå…¨ä¸€è‡´
            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 44100.0,
                AVNumberOfChannelsKey: 2,
                AVEncoderBitRateKey: 128000
            ]
            
            m4aAudioWriterInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            m4aAudioWriterInput?.expectsMediaDataInRealTime = true
            
            if let input = m4aAudioWriterInput {
                m4aAudioWriter?.add(input)
                m4aAudioWriter?.startWriting()
            }
            
            logger.info("ğŸ™ï¸ å¼€å§‹å½•åˆ¶éŸ³é¢‘: \(fileName)")
            
            // é€šçŸ¥ä¸»ç¨‹åºæ–°æ–‡ä»¶å·²åˆ›å»º
            notifyM4AAudioFileCreated(fileName: fileName, fileURL: audioFileURL)
            
        } catch {
            logger.error("âŒ åˆ›å»ºéŸ³é¢‘å†™å…¥å™¨å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    // å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„saveAudioToFileæ–¹æ³•
    private func saveOriginalAudioToFile(_ sampleBuffer: CMSampleBuffer) {
        guard let writer = m4aAudioWriter,
              let input = m4aAudioWriterInput else {
            logger.error("âŒ m4aAudioWriter æˆ– m4aAudioWriterInput ä¸º nil")
            return
        }
        
        guard writer.status == .writing else {
            logger.error("âŒ AVAssetWriterçŠ¶æ€ä¸æ˜¯writing: \(writer.status.rawValue)")
            return
        }
        
        guard input.isReadyForMoreMediaData else {
            logger.warning("âš ï¸ AVAssetWriterInput ä¸å‡†å¤‡æ¥æ”¶æ›´å¤šæ•°æ®")
            return
        }
        
        // è®¾ç½®å¼€å§‹æ—¶é—´
        if m4aStartTime == nil {
            m4aStartTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)
            logger.info("ğŸµ å‡†å¤‡å¼€å§‹M4Aå½•åˆ¶ä¼šè¯ï¼Œæ—¶é—´æˆ³: \(CMTimeGetSeconds(self.m4aStartTime!))")
            
            // ç¡®ä¿æ—¶é—´æˆ³æœ‰æ•ˆ
            guard CMTIME_IS_VALID(m4aStartTime!) && CMTIME_IS_NUMERIC(m4aStartTime!) else {
                logger.error("âŒ æ— æ•ˆçš„å¼€å§‹æ—¶é—´æˆ³")
                m4aStartTime = nil
                return
            }
            
            writer.startSession(atSourceTime: m4aStartTime!)
            logger.info("âœ… M4Aå½•åˆ¶ä¼šè¯å·²å¼€å§‹")
        }
        
        // å†™å…¥éŸ³é¢‘æ•°æ® - å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandler
        let success = input.append(sampleBuffer)
        if success {
            logger.debug("âœ… æˆåŠŸå†™å…¥éŸ³é¢‘æ•°æ®åˆ°M4Aæ–‡ä»¶")
        } else {
            logger.error("âŒ å†™å…¥éŸ³é¢‘æ•°æ®åˆ°M4Aæ–‡ä»¶å¤±è´¥")
        }
    }
    
    // ä»åŸå§‹éŸ³é¢‘æ•°æ®é‡å»ºCMSampleBuffer
    private func createSampleBufferFromData(_ data: Data, formatInfo: [String: Any]) -> CMSampleBuffer? {
        guard let sampleRate = formatInfo["sampleRate"] as? Double,
              let channels = formatInfo["channels"] as? UInt32,
              let formatID = formatInfo["formatID"] as? UInt32,
              let formatFlags = formatInfo["formatFlags"] as? UInt32,  // å…³é”®ï¼šè¯»å–formatFlags
              let bitsPerChannel = formatInfo["bitsPerChannel"] as? UInt32,
              let bytesPerFrame = formatInfo["bytesPerFrame"] as? UInt32,
              let framesPerPacket = formatInfo["framesPerPacket"] as? UInt32,
              let bytesPerPacket = formatInfo["bytesPerPacket"] as? UInt32 else {
            logger.error("âŒ éŸ³é¢‘æ ¼å¼ä¿¡æ¯ä¸å®Œæ•´: \(formatInfo)")
            return nil
        }
        
        logger.info("ğŸ” é‡å»ºCMSampleBuffer - æ•°æ®å¤§å°: \(data.count)bytes, æ ¼å¼: \(sampleRate)Hz, \(channels)å£°é“, \(bitsPerChannel)ä½, formatID: \(formatID), flags: \(formatFlags)")
        
        // åˆ›å»ºéŸ³é¢‘æµåŸºæœ¬æè¿°
        var asbd = AudioStreamBasicDescription()
        asbd.mSampleRate = sampleRate
        asbd.mFormatID = formatID
        asbd.mChannelsPerFrame = channels
        asbd.mBitsPerChannel = bitsPerChannel
        asbd.mBytesPerFrame = bytesPerFrame
        asbd.mFramesPerPacket = framesPerPacket
        asbd.mBytesPerPacket = bytesPerPacket
        // å…³é”®ï¼šç›´æ¥ä½¿ç”¨ä»æ‰©å±•ç¨‹åºå‘é€çš„åŸå§‹formatFlags
        asbd.mFormatFlags = formatFlags
        
        logger.info("ğŸµ ä½¿ç”¨åŸå§‹éŸ³é¢‘æ ¼å¼æ ‡å¿—: \(formatFlags)")
        
        // åˆ›å»ºéŸ³é¢‘æ ¼å¼æè¿°
        var formatDescription: CMAudioFormatDescription?
        let formatStatus = CMAudioFormatDescriptionCreate(
            allocator: kCFAllocatorDefault,
            asbd: &asbd,
            layoutSize: 0,
            layout: nil,
            magicCookieSize: 0,
            magicCookie: nil,
            extensions: nil,
            formatDescriptionOut: &formatDescription
        )
        
        guard formatStatus == noErr, let audioFormatDescription = formatDescription else {
            logger.error("âŒ åˆ›å»ºéŸ³é¢‘æ ¼å¼æè¿°å¤±è´¥: \(formatStatus)")
            return nil
        }
        
        // åˆ›å»ºCMBlockBuffer - ä½¿ç”¨æ‹·è´æ–¹å¼ç¡®ä¿æ•°æ®å®‰å…¨
        var blockBuffer: CMBlockBuffer?
        let blockBufferStatus = CMBlockBufferCreateWithMemoryBlock(
            allocator: kCFAllocatorDefault,
            memoryBlock: nil,  // è®©ç³»ç»Ÿåˆ†é…å†…å­˜
            blockLength: data.count,
            blockAllocator: kCFAllocatorDefault,
            customBlockSource: nil,
            offsetToData: 0,
            dataLength: data.count,
            flags: 0,
            blockBufferOut: &blockBuffer
        )
        
        // å°†æ•°æ®æ‹·è´åˆ°CMBlockBufferä¸­
        if blockBufferStatus == noErr, let audioBlockBuffer = blockBuffer {
            let copyStatus = data.withUnsafeBytes { dataPtr in
                CMBlockBufferReplaceDataBytes(
                    with: dataPtr.baseAddress!,
                    blockBuffer: audioBlockBuffer,
                    offsetIntoDestination: 0,
                    dataLength: data.count
                )
            }
            
            if copyStatus != noErr {
                logger.error("âŒ æ‹·è´éŸ³é¢‘æ•°æ®åˆ°CMBlockBufferå¤±è´¥: \(copyStatus)")
                return nil
            }
        }
        
        guard blockBufferStatus == noErr, let audioBlockBuffer = blockBuffer else {
            logger.error("âŒ åˆ›å»ºCMBlockBufferå¤±è´¥: \(blockBufferStatus)")
            return nil
        }
        
        // åˆ›å»ºæ—¶é—´æˆ³ä¿¡æ¯
        let frameCount = data.count / Int(bytesPerFrame)
        var sampleTiming = CMSampleTimingInfo()
        sampleTiming.duration = CMTime(value: CMTimeValue(frameCount), timescale: CMTimeScale(sampleRate))
        sampleTiming.presentationTimeStamp = CMTime(value: CMTimeValue(Date().timeIntervalSince1970 * sampleRate), timescale: CMTimeScale(sampleRate))
        sampleTiming.decodeTimeStamp = CMTime.invalid
        
        // åˆ›å»ºCMSampleBuffer
        var sampleBuffer: CMSampleBuffer?
        let sampleBufferStatus = CMSampleBufferCreate(
            allocator: kCFAllocatorDefault,
            dataBuffer: audioBlockBuffer,
            dataReady: true,
            makeDataReadyCallback: nil,
            refcon: nil,
            formatDescription: audioFormatDescription,
            sampleCount: CMItemCount(frameCount),
            sampleTimingEntryCount: 1,
            sampleTimingArray: &sampleTiming,
            sampleSizeEntryCount: 0,
            sampleSizeArray: nil,
            sampleBufferOut: &sampleBuffer
        )
        
        guard sampleBufferStatus == noErr else {
            logger.error("âŒ åˆ›å»ºCMSampleBufferå¤±è´¥: \(sampleBufferStatus)")
            return nil
        }
        
        return sampleBuffer
    }
    
    // MARK: - Speech Recognition with CMSampleBuffer
    
    private func performSpeechRecognitionWithSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
        guard let recognitionRequest = recognitionRequest else {
            logger.warning("âš ï¸ è¯­éŸ³è¯†åˆ«è¯·æ±‚ä¸ºç©º")
            return
        }
        
        // ä» CMSampleBuffer åˆ›å»º AVAudioPCMBuffer ç”¨äºè¯­éŸ³è¯†åˆ«
        guard let audioBuffer = createAudioPCMBufferFromSampleBuffer(sampleBuffer) else {
            logger.warning("âš ï¸ ä»CMSampleBufferåˆ›å»ºAVAudioPCMBufferå¤±è´¥")
            return
        }
        
        // æ£€æŸ¥éŸ³é¢‘æ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯é™éŸ³ï¼‰
        let audioLevel = calculateSimpleAudioLevel(from: audioBuffer)
        logger.info("ğŸµ è¯­éŸ³è¯†åˆ«éŸ³é¢‘ç”µå¹³: \(String(format: "%.6f", audioLevel))")
        
        if audioLevel < 0.001 {
            logger.warning("âš ï¸ éŸ³é¢‘ç”µå¹³å¤ªä½ï¼Œå¯èƒ½æ˜¯é™éŸ³æ•°æ®")
        }
        
        // ä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®ç”¨äºéªŒè¯
        saveConvertedAudioBuffer(audioBuffer)
        
        // å‘é€åˆ°è¯­éŸ³è¯†åˆ«å™¨
        recognitionRequest.append(audioBuffer)
        
        logger.debug("âœ… ä½¿ç”¨é‡å»ºçš„CMSampleBufferè¿›è¡Œè¯­éŸ³è¯†åˆ« (ç”µå¹³: \(String(format: "%.6f", audioLevel)))")
    }
    
    private func calculateSimpleAudioLevel(from audioBuffer: AVAudioPCMBuffer) -> Double {
        guard audioBuffer.frameLength > 0 else { return 0.0 }
        
        let format = audioBuffer.format
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(format.channelCount)
        
        var sum: Double = 0.0
        var sampleCount = 0
        
        // å¤„ç†äº¤é”™æ ¼å¼çš„éŸ³é¢‘æ•°æ®
        if let audioData = audioBuffer.audioBufferList.pointee.mBuffers.mData {
            if format.commonFormat == .pcmFormatInt16 {
                // 16ä½æ•´æ•°æ ¼å¼
                let int16Pointer = audioData.bindMemory(to: Int16.self, capacity: frameCount * channels)
                for i in 0..<(frameCount * channels) {
                    let sample = Double(int16Pointer[i]) / 32768.0
                    sum += sample * sample
                    sampleCount += 1
                }
            } else if format.commonFormat == .pcmFormatFloat32 {
                // 32ä½æµ®ç‚¹æ ¼å¼
                let floatPointer = audioData.bindMemory(to: Float.self, capacity: frameCount * channels)
                for i in 0..<(frameCount * channels) {
                    let sample = Double(floatPointer[i])
                    sum += sample * sample
                    sampleCount += 1
                }
            }
        }
        
        guard sampleCount > 0 else { return 0.0 }
        
        let rms = sqrt(sum / Double(sampleCount))
        return rms
    }
    
    private func createAudioPCMBufferFromSampleBuffer(_ sampleBuffer: CMSampleBuffer) -> AVAudioPCMBuffer? {
        guard let formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer) else {
            logger.error("âŒ æ— æ³•è·å–CMSampleBufferæ ¼å¼æè¿°")
            return nil
        }
        
        let streamDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription)
        guard let audioStreamDescription = streamDescription else {
            logger.error("âŒ æ— æ³•è·å–éŸ³é¢‘æµæè¿°")
            return nil
        }
        
        logger.info("ğŸ” è¾“å…¥éŸ³é¢‘æ ¼å¼: \(audioStreamDescription.pointee.mSampleRate)Hz, \(audioStreamDescription.pointee.mChannelsPerFrame)å£°é“, \(audioStreamDescription.pointee.mBitsPerChannel)ä½")
        
        // ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ ¼å¼è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œä¸è¿›è¡Œå¤æ‚çš„æ ¼å¼è½¬æ¢
        guard let inputAVFormat = AVAudioFormat(streamDescription: audioStreamDescription) else {
            logger.error("âŒ åˆ›å»ºè¾“å…¥éŸ³é¢‘æ ¼å¼å¤±è´¥")
            return nil
        }
        
        // è·å–è¾“å…¥æ•°æ®
        guard let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) else {
            logger.error("âŒ æ— æ³•è·å–CMSampleBufferæ•°æ®ç¼“å†²åŒº")
            return nil
        }
        
        let dataLength = CMBlockBufferGetDataLength(blockBuffer)
        let inputFrameCount = AVAudioFrameCount(dataLength / Int(audioStreamDescription.pointee.mBytesPerFrame))
        
        logger.info("ğŸ” éŸ³é¢‘æ•°æ®: é•¿åº¦=\(dataLength)å­—èŠ‚, å¸§æ•°=\(inputFrameCount)")
        
        guard inputFrameCount > 0 else {
            logger.warning("âš ï¸ éŸ³é¢‘å¸§æ•°ä¸º0ï¼Œè·³è¿‡å¤„ç†")
            return nil
        }
        
        guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: inputAVFormat, frameCapacity: inputFrameCount) else {
            logger.error("âŒ åˆ›å»ºè¾“å…¥PCMç¼“å†²åŒºå¤±è´¥")
            return nil
        }
        
        // å¤åˆ¶æ•°æ®åˆ°è¾“å…¥ç¼“å†²åŒº
        var dataPointer: UnsafeMutablePointer<Int8>?
        let result = CMBlockBufferGetDataPointer(blockBuffer, atOffset: 0, lengthAtOffsetOut: nil, totalLengthOut: nil, dataPointerOut: &dataPointer)
        
        guard result == noErr, let data = dataPointer else {
            logger.error("âŒ æ— æ³•è·å–éŸ³é¢‘æ•°æ®æŒ‡é’ˆ: \(result)")
            return nil
        }
        
        // ç›´æ¥å¤åˆ¶éŸ³é¢‘æ•°æ®
        let audioBufferList = inputBuffer.mutableAudioBufferList
        audioBufferList.pointee.mBuffers.mData?.copyMemory(from: data, byteCount: dataLength)
        audioBufferList.pointee.mBuffers.mDataByteSize = UInt32(dataLength)
        inputBuffer.frameLength = inputFrameCount
        
        logger.info("âœ… éŸ³é¢‘PCMç¼“å†²åŒºåˆ›å»ºæˆåŠŸ: \(inputFrameCount)å¸§")
        
        return inputBuffer
    }
    
    // å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„stopAudioRecordingæ–¹æ³•
    private func stopM4ARecording() {
        guard let writer = m4aAudioWriter else { return }
        
        m4aAudioWriterInput?.markAsFinished()
        
        // ä¿å­˜URLçš„å‰¯æœ¬ï¼Œé˜²æ­¢åœ¨å¼‚æ­¥å—ä¸­è¢«æ¸…ç©º
        let audioFileURL = currentM4AFileURL
        
        writer.finishWriting { [weak self] in
            if writer.status == .completed {
                self?.logger.info("âœ… éŸ³é¢‘æ–‡ä»¶å½•åˆ¶å®Œæˆ")
                if let url = audioFileURL {
                    Task { @MainActor in
                        self?.notifyM4AAudioFileCompleted(fileURL: url)
                    }
                    self?.logger.info("ğŸ“ éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: \(url.lastPathComponent)")
                }
            } else if let error = writer.error {
                self?.logger.error("âŒ éŸ³é¢‘æ–‡ä»¶å†™å…¥å¤±è´¥: \(error.localizedDescription)")
            }
            
            // æ¸…ç†å¼•ç”¨
            Task { @MainActor in
                self?.m4aAudioWriter = nil
                self?.m4aAudioWriterInput = nil
                self?.currentM4AFileURL = nil
                self?.m4aStartTime = nil  // ç¡®ä¿é‡ç½®å¼€å§‹æ—¶é—´
            }
        }
    }
    
    private func notifyM4AAudioFileCreated(fileName: String, fileURL: URL) {
        let notification: [String: Any] = [
            "event": "audio_file_created",
            "fileName": fileName,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("realtime_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            logger.info("ğŸ“¡ å·²é€šçŸ¥M4AéŸ³é¢‘æ–‡ä»¶åˆ›å»º")
        } catch {
            logger.error("âŒ å†™å…¥M4AéŸ³é¢‘åˆ›å»ºé€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func notifyM4AAudioFileCompleted(fileURL: URL) {
        let notification: [String: Any] = [
            "event": "audio_file_completed",
            "fileName": fileURL.lastPathComponent,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("realtime_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            
            // å‘é€Darwiné€šçŸ¥
            let darwinCenter = CFNotificationCenterGetDarwinNotifyCenter()
            let notificationName = CFNotificationName("dev.tuist.Siri.realtimeAudioSaved" as CFString)
            CFNotificationCenterPostNotification(darwinCenter, notificationName, nil, nil, true)
            
            logger.info("ğŸ“¡ å·²é€šçŸ¥M4AéŸ³é¢‘æ–‡ä»¶å®Œæˆ")
        } catch {
            logger.error("âŒ å†™å…¥M4AéŸ³é¢‘å®Œæˆé€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
}

extension RealtimeAudioStreamManager: SFSpeechRecognizerDelegate {
    nonisolated public func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        Task { @MainActor in
            logger.info("ğŸ¤ è¯­éŸ³è¯†åˆ«å™¨å¯ç”¨æ€§å˜åŒ–: \(available)")
            if !available && isProcessing {
                stopRecognition()
            }
        }
    }
}
