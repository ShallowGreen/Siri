import Foundation
import AVFoundation
import Speech
import os.log

@MainActor
public class RealtimeAudioStreamManager: NSObject, ObservableObject {
    
    @Published public var isProcessing: Bool = false
    @Published public var recognizedText: String = ""
    @Published public var errorMessage: String = ""
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let logger = Logger(subsystem: "dev.tuist.Siri", category: "RealtimeAudio")
    private let appGroupID = "group.dev.tuist.Siri"
    
    private var darwinNotificationCenter: CFNotificationCenter?
    private var audioBufferQueue = [CMSampleBuffer]()
    
    // ç”¨äºä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®è¿›è¡ŒéªŒè¯
    private var audioWriter: AVAssetWriter?
    private var audioWriterInput: AVAssetWriterInput?
    private var convertedAudioFileURL: URL?
    private var processingQueue = DispatchQueue(label: "realtime.audio.processing", qos: .userInitiated)
    private var hasLoggedFormat = false
    
    public override init() {
        super.init()
        // æš‚æ—¶ç§»é™¤éŸ³é¢‘ä¼šè¯è®¾ç½®ï¼Œé¿å…å¹²æ‰°åŸæœ‰å½•åˆ¶åŠŸèƒ½
        // setupAudioSession()
        speechRecognizer?.delegate = self
        setupDarwinNotifications()
    }
    
    public func startMonitoring() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            errorMessage = "è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨"
            return
        }
        
        SFSpeechRecognizer.requestAuthorization { [weak self] authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    self?.startRecognition()
                case .denied:
                    self?.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»"
                case .restricted:
                    self?.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™å—é™"
                case .notDetermined:
                    self?.errorMessage = "è¯­éŸ³è¯†åˆ«æƒé™æœªç¡®å®š"
                @unknown default:
                    self?.errorMessage = "æœªçŸ¥çš„æƒé™çŠ¶æ€"
                }
            }
        }
    }
    
    public func stopMonitoring() {
        logger.info("ğŸ›‘ åœæ­¢å®æ—¶éŸ³é¢‘æµç›‘æ§")
        stopRecognition()
    }
    
    private func setupDarwinNotifications() {
        darwinNotificationCenter = CFNotificationCenterGetDarwinNotifyCenter()
        
        let notificationName = "dev.tuist.Siri.audiodata" as CFString
        let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque())
        
        CFNotificationCenterAddObserver(
            darwinNotificationCenter,
            observer,
            { (center, observer, name, object, userInfo) in
                guard let observer = observer else { return }
                let manager = Unmanaged<RealtimeAudioStreamManager>.fromOpaque(observer).takeUnretainedValue()
                Task { @MainActor in
                    manager.handleAudioDataNotification()
                }
            },
            notificationName,
            nil,
            .deliverImmediately
        )
        
        logger.info("ğŸ“¡ Darwiné€šçŸ¥ç›‘å¬å·²è®¾ç½®")
    }
    
    private func handleAudioDataNotification() {
        logger.debug("ğŸ“± æ”¶åˆ°éŸ³é¢‘æ•°æ®é€šçŸ¥")
        readAudioDataFromSharedMemory()
    }
    
    private func readAudioDataFromSharedMemory() {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let audioDataURL = containerURL.appendingPathComponent("realtime_audio_buffer.data")
        let formatURL = containerURL.appendingPathComponent("realtime_audio_format.json")
        
        guard FileManager.default.fileExists(atPath: audioDataURL.path),
              FileManager.default.fileExists(atPath: formatURL.path),
              let audioData = try? Data(contentsOf: audioDataURL),
              let formatData = try? Data(contentsOf: formatURL),
              let formatInfo = try? JSONSerialization.jsonObject(with: formatData) as? [String: Any] else {
            return
        }
        
        processingQueue.async { [weak self] in
            Task { @MainActor in
                self?.processAudioData(audioData, formatInfo: formatInfo)
            }
        }
    }
    
    private func processAudioData(_ data: Data, formatInfo: [String: Any]) {
        guard isProcessing,
              let recognitionRequest = recognitionRequest else {
            return
        }
        
        // ä»æ ¼å¼ä¿¡æ¯ä¸­è·å–éŸ³é¢‘å‚æ•°
        guard let sampleRate = formatInfo["sampleRate"] as? Double,
              let channels = formatInfo["channels"] as? UInt32,
              let formatID = formatInfo["formatID"] as? UInt32,
              let bitsPerChannel = formatInfo["bitsPerChannel"] as? UInt32 else {
            logger.error("âŒ æ— æ³•è§£æéŸ³é¢‘æ ¼å¼ä¿¡æ¯")
            return
        }
        
        // åªåœ¨é¦–æ¬¡è¯†åˆ«æ ¼å¼æ—¶è®°å½•
        if !hasLoggedFormat && (formatID == kAudioFormatLinearPCM || formatID == 1819304813) {
            logger.info("ğŸµ PCMæ ¼å¼: \(sampleRate)Hz, \(channels)å£°é“, \(bitsPerChannel)ä½")
            hasLoggedFormat = true
        }
        
        // åˆ›å»ºåˆé€‚çš„éŸ³é¢‘æ ¼å¼
        var audioFormat: AVAudioFormat?
        
        // æ ¹æ®å®é™…æ ¼å¼åˆ›å»ºAVAudioFormat
        // kAudioFormatLinearPCM = 1819304813 ('lpcm') æ˜¯çº¿æ€§PCMæ ¼å¼çš„æ ‡è¯†ç¬¦
        if formatID == kAudioFormatLinearPCM || formatID == 1819304813 {
            // PCMæ ¼å¼ - ä½¿ç”¨éäº¤é”™æ ¼å¼ï¼ˆå‚è€ƒæ­£ç¡®çš„ demoï¼‰
            if bitsPerChannel == 32 {
                // 32ä½æµ®ç‚¹ - éäº¤é”™æ ¼å¼
                audioFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: AVAudioChannelCount(channels), interleaved: false)
            } else if bitsPerChannel == 16 {
                // 16ä½æ•´æ•° - éäº¤é”™æ ¼å¼
                audioFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: sampleRate, channels: AVAudioChannelCount(channels), interleaved: false)
            }
        }
        
        guard let format = audioFormat else {
            logger.error("âŒ ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: formatID=\(formatID), bitsPerChannel=\(bitsPerChannel)")
            return
        }
        
        // è®¡ç®—å¸§æ•°é‡
        let bytesPerSample = bitsPerChannel / 8
        let frameCount = AVAudioFrameCount(data.count / (Int(bytesPerSample) * Int(channels)))
        
        guard frameCount > 0 else {
            return
        }
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
            return
        }
        
        audioBuffer.frameLength = frameCount
        
        // å¤åˆ¶éŸ³é¢‘æ•°æ® - ä½¿ç”¨éäº¤é”™æ ¼å¼ï¼ˆå‚è€ƒ demo çš„æ­£ç¡®åšæ³•ï¼‰
        data.withUnsafeBytes { rawBytes in
            if bitsPerChannel == 16 {
                // 16ä½æ•´æ•°æ•°æ® - ä»äº¤é”™è½¬ä¸ºéäº¤é”™
                guard let int16Pointer = rawBytes.bindMemory(to: Int16.self).baseAddress,
                      let channelData = audioBuffer.int16ChannelData else {
                    return
                }
                
                if channels == 2 {
                    // ç«‹ä½“å£°ï¼šå°†äº¤é”™æ•°æ®åˆ†ç¦»åˆ°ä¸¤ä¸ªé€šé“ï¼ˆæŒ‰ demo æ–¹å¼ï¼‰
                    let leftChannel = channelData[0]
                    let rightChannel = channelData[1]
                    
                    for frame in 0..<Int(frameCount) {
                        let interleavedIndex = frame * 2
                        leftChannel[frame] = int16Pointer[interleavedIndex]     // å·¦å£°é“
                        rightChannel[frame] = int16Pointer[interleavedIndex + 1] // å³å£°é“
                    }
                    
                    let firstLeft = leftChannel[0]
                    let firstRight = rightChannel[0]
                    logger.info("ğŸ” éäº¤é”™è½¬æ¢: L=\(firstLeft), R=\(firstRight), å¸§æ•°=\(frameCount)")
                    
                } else {
                    // å•å£°é“ï¼šç›´æ¥å¤åˆ¶ï¼ˆåƒDemoä¸€æ ·ï¼‰
                    let channel = channelData[0]
                    channel.initialize(from: int16Pointer, count: Int(frameCount))
                    logger.info("ğŸ” å•å£°é“å¤åˆ¶: é¦–æ ·æœ¬=\(channel[0]), å¸§æ•°=\(frameCount)")
                }
                
            } else if bitsPerChannel == 32 {
                // 32ä½æµ®ç‚¹æ•°æ® - ä»äº¤é”™è½¬ä¸ºéäº¤é”™
                guard let floatPointer = rawBytes.bindMemory(to: Float.self).baseAddress,
                      let channelData = audioBuffer.floatChannelData else {
                    return
                }
                
                if channels == 2 {
                    // ç«‹ä½“å£°ï¼šåˆ†ç¦»äº¤é”™æ•°æ®
                    let leftChannel = channelData[0]
                    let rightChannel = channelData[1]
                    
                    for frame in 0..<Int(frameCount) {
                        let interleavedIndex = frame * 2
                        leftChannel[frame] = floatPointer[interleavedIndex]
                        rightChannel[frame] = floatPointer[interleavedIndex + 1]
                    }
                } else {
                    // å•å£°é“ï¼šç›´æ¥å¤åˆ¶
                    let channel = channelData[0]
                    channel.initialize(from: floatPointer, count: Int(frameCount))
                }
            }
        }
        
        // ä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®ç”¨äºéªŒè¯
        // saveConvertedAudioBuffer(audioBuffer)  // æš‚æ—¶æ³¨é‡Šæ‰ï¼Œåªæµ‹è¯•M4Aåˆ°WAVè½¬æ¢
        
        // å‘é€åˆ°è¯­éŸ³è¯†åˆ«å™¨
        recognitionRequest.append(audioBuffer)
    }
    
    private func calculateAudioLevel(from audioBuffer: AVAudioPCMBuffer) -> Double {
        guard let channelData = audioBuffer.int16ChannelData?[0] else {
            return 0.0
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(audioBuffer.format.channelCount)
        
        var sum: Double = 0.0
        let sampleCount = frameCount * channels
        
        for i in 0..<sampleCount {
            let sample = Double(channelData[i]) / 32768.0 // å½’ä¸€åŒ–åˆ° -1.0 åˆ° 1.0
            sum += sample * sample
        }
        
        let rms = sqrt(sum / Double(sampleCount))
        return rms
    }
    
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            // ä½¿ç”¨ playback æ¨¡å¼ï¼Œä¿æŒåŸæœ‰çš„æ‰¬å£°å™¨è¾“å‡ºï¼ŒåŒæ—¶æ”¯æŒä¸å…¶ä»–éŸ³é¢‘æ··åˆ
            try audioSession.setCategory(.playback, mode: .default, options: [.mixWithOthers, .duckOthers])
            logger.info("ğŸµ éŸ³é¢‘ä¼šè¯è®¾ç½®æˆåŠŸ (playback + default)")
        } catch {
            logger.error("âŒ éŸ³é¢‘ä¼šè¯è®¾ç½®å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func startRecognition() {
        logger.info("ğŸš€ å¼€å§‹è¯­éŸ³è¯†åˆ«")
        
        guard !isProcessing else {
            return
        }
        
        isProcessing = true
        hasLoggedFormat = false
        
        // å¼€å§‹å½•åˆ¶è½¬æ¢åçš„éŸ³é¢‘ç”¨äºéªŒè¯
        // startConvertedAudioRecording()  // æš‚æ—¶æ³¨é‡Šæ‰ï¼Œåªæµ‹è¯•M4Aåˆ°WAVè½¬æ¢
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            errorMessage = "æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚"
            isProcessing = false
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            DispatchQueue.main.async {
                if let result = result {
                    let newText = result.bestTranscription.formattedString
                    
                    if !newText.isEmpty {
                        self?.recognizedText = newText
                        self?.logger.info("ğŸ¯ è¯†åˆ«: \(newText)")
                    }
                    
                    // åˆ é™¤æœ€ç»ˆç»“æœæ—¥å¿—ï¼Œå‡å°‘è¾“å‡º
                }
                
                if let error = error {
                    self?.logger.error("âŒ è¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
                    // æ£€æŸ¥æ˜¯å¦æ˜¯"No speech detected"é”™è¯¯
                    if error.localizedDescription.contains("No speech") {
                        self?.logger.info("âš ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³ - å¯èƒ½éŸ³é¢‘å†…å®¹ä¸ºé™éŸ³æˆ–éŸ³é‡è¿‡ä½")
                    }
                    self?.errorMessage = error.localizedDescription
                    self?.stopRecognition()
                }
            }
        }
        
        logger.info("âœ… è¯­éŸ³è¯†åˆ«ä»»åŠ¡å·²å¯åŠ¨")
    }
    
    private func stopRecognition() {
        logger.info("ğŸ›‘ åœæ­¢è¯­éŸ³è¯†åˆ«")
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        isProcessing = false
        
        // åœæ­¢å½•åˆ¶è½¬æ¢åçš„éŸ³é¢‘
        // stopConvertedAudioRecording()  // æš‚æ—¶æ³¨é‡Šæ‰ï¼Œåªæµ‹è¯•M4Aåˆ°WAVè½¬æ¢
        
        logger.info("âœ… è¯­éŸ³è¯†åˆ«å·²åœæ­¢")
    }
    
    deinit {
        let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque())
        CFNotificationCenterRemoveObserver(
            darwinNotificationCenter,
            observer,
            CFNotificationName("dev.tuist.Siri.audiodata" as CFString),
            nil
        )
    }
    
    // MARK: - Converted Audio Recording for Verification
    
    private func startConvertedAudioRecording() {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        // åˆ›å»ºéŸ³é¢‘ç›®å½•
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        if !FileManager.default.fileExists(atPath: audioDirectory.path) {
            do {
                try FileManager.default.createDirectory(at: audioDirectory, withIntermediateDirectories: true, attributes: nil)
            } catch {
                return
            }
        }
        
        // åˆ›å»ºè½¬æ¢åéŸ³é¢‘æ–‡ä»¶
        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium).replacingOccurrences(of: ":", with: "-")
        let fileName = "ConvertedAudio_\(timestamp).m4a"
        convertedAudioFileURL = audioDirectory.appendingPathComponent(fileName)
        
        guard let audioFileURL = convertedAudioFileURL else { return }
        
        // è®¾ç½®éŸ³é¢‘å†™å…¥å™¨
        do {
            audioWriter = try AVAssetWriter(outputURL: audioFileURL, fileType: .m4a)
            
            // é…ç½®éŸ³é¢‘è®¾ç½® - ä½¿ç”¨ä¸è½¬æ¢åç›¸åŒçš„æ ¼å¼
            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 44100.0,
                AVNumberOfChannelsKey: 2,
                AVEncoderBitRateKey: 128000
            ]
            
            audioWriterInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            audioWriterInput?.expectsMediaDataInRealTime = true
            
            if let input = audioWriterInput {
                audioWriter?.add(input)
                audioWriter?.startWriting()
            }
            
            logger.info("ğŸ™ï¸ å¼€å§‹å½•åˆ¶è½¬æ¢åéŸ³é¢‘: \(fileName)")
            
        } catch {
            logger.error("âŒ åˆ›å»ºè½¬æ¢éŸ³é¢‘å†™å…¥å™¨å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func saveConvertedAudioBuffer(_ audioBuffer: AVAudioPCMBuffer) {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID),
              let int16Data = audioBuffer.int16ChannelData else {
            return
        }
        
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        let tempDataURL = audioDirectory.appendingPathComponent("converted_audio_temp.pcm")
        
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(audioBuffer.format.channelCount)
        let interleavedDataSize = frameCount * channels * MemoryLayout<Int16>.size
        
        // å°†éäº¤é”™æ•°æ®è½¬æ¢ä¸ºäº¤é”™æ ¼å¼ï¼ˆWAVæ ‡å‡†ï¼Œå‚è€ƒdemoæ­£ç¡®åšæ³•ï¼‰
        var interleavedData = Data(capacity: interleavedDataSize)
        
        if channels == 2 {
            // ç«‹ä½“å£°ï¼šäº¤é”™å·¦å³å£°é“
            let leftChannel = int16Data[0]
            let rightChannel = int16Data[1]
            
            for frame in 0..<frameCount {
                // äº¤é”™æ ¼å¼ï¼šL, R, L, R, ...
                withUnsafeBytes(of: leftChannel[frame]) { interleavedData.append(contentsOf: $0) }
                withUnsafeBytes(of: rightChannel[frame]) { interleavedData.append(contentsOf: $0) }
            }
            
            logger.info("ğŸ’¾ ä¿å­˜ç«‹ä½“å£°æ•°æ®: Lé¦–æ ·æœ¬=\(leftChannel[0]), Ré¦–æ ·æœ¬=\(rightChannel[0]), å¸§æ•°=\(frameCount)")
            
        } else {
            // å•å£°é“ï¼šç›´æ¥å¤åˆ¶ï¼ˆå‚è€ƒdemoæ–¹å¼ï¼‰
            let channel = int16Data[0]
            for frame in 0..<frameCount {
                withUnsafeBytes(of: channel[frame]) { interleavedData.append(contentsOf: $0) }
            }
            
            logger.info("ğŸ’¾ ä¿å­˜å•å£°é“æ•°æ®: é¦–æ ·æœ¬=\(channel[0]), å¸§æ•°=\(frameCount)")
        }
        
        
        do {
            if FileManager.default.fileExists(atPath: tempDataURL.path) {
                let fileHandle = try FileHandle(forWritingTo: tempDataURL)
                defer { try? fileHandle.close() }
                _ = try fileHandle.seekToEnd()
                try fileHandle.write(contentsOf: interleavedData)
            } else {
                try interleavedData.write(to: tempDataURL)
            }
        } catch {
            // ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½
        }
    }
    
    
    private func stopConvertedAudioRecording() {
        // å°†ä¸´æ—¶PCMæ•°æ®è½¬æ¢ä¸ºå¯æ’­æ”¾çš„éŸ³é¢‘æ–‡ä»¶
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        let tempDataURL = audioDirectory.appendingPathComponent("converted_audio_temp.pcm")
        
        // æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        guard FileManager.default.fileExists(atPath: tempDataURL.path) else {
            logger.info("âš ï¸ æ²¡æœ‰è½¬æ¢åçš„éŸ³é¢‘æ•°æ®éœ€è¦ä¿å­˜")
            return
        }
        
        // åˆ›å»ºæœ€ç»ˆçš„éŸ³é¢‘æ–‡ä»¶å
        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium).replacingOccurrences(of: ":", with: "-")
        let fileName = "ConvertedAudio_\(timestamp).wav"
        let finalURL = audioDirectory.appendingPathComponent(fileName)
        
        // å°†PCMæ•°æ®è½¬æ¢ä¸ºWAVæ–‡ä»¶
        convertPCMToWAV(inputURL: tempDataURL, outputURL: finalURL)
        
        // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try? FileManager.default.removeItem(at: tempDataURL)
        
        audioWriter = nil
        audioWriterInput = nil
        convertedAudioFileURL = nil
        
        logger.info("âœ… è½¬æ¢åéŸ³é¢‘æ–‡ä»¶å½•åˆ¶å®Œæˆ: \(fileName)")
        notifyConvertedAudioFileCompleted(fileURL: finalURL)
    }
    
    private func convertPCMToWAV(inputURL: URL, outputURL: URL) {
        do {
            let pcmData = try Data(contentsOf: inputURL)
            guard pcmData.count > 0 else {
                logger.error("âŒ PCMæ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºWAVæ–‡ä»¶")
                return
            }
            
            let wavData = createWAVHeader(dataLength: pcmData.count) + pcmData
            try wavData.write(to: outputURL)
            logger.info("ğŸ“ WAVæ–‡ä»¶å·²åˆ›å»º: \(outputURL.lastPathComponent), å¤§å°: \(wavData.count) bytes (PCM: \(pcmData.count) bytes)")
        } catch {
            logger.error("âŒ è½¬æ¢PCMä¸ºWAVå¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func createWAVHeader(dataLength: Int) -> Data {
        var header = Data()
        
        // RIFF header (12 bytes)
        header.append("RIFF".data(using: .ascii)!)                                    // "RIFF" (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(36 + dataLength).littleEndian) { Data($0) }) // File size - 8 (4 bytes)
        header.append("WAVE".data(using: .ascii)!)                                    // "WAVE" (4 bytes)
        
        // fmt sub-chunk (24 bytes)
        header.append("fmt ".data(using: .ascii)!)                                    // "fmt " (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(16).littleEndian) { Data($0) })      // fmt chunk size (4 bytes)
        header.append(withUnsafeBytes(of: UInt16(1).littleEndian) { Data($0) })       // PCM = 1 (2 bytes)
        header.append(withUnsafeBytes(of: UInt16(2).littleEndian) { Data($0) })       // 2 channels (2 bytes)
        header.append(withUnsafeBytes(of: UInt32(44100).littleEndian) { Data($0) })   // Sample rate (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(44100 * 2 * 2).littleEndian) { Data($0) }) // Byte rate (4 bytes)
        header.append(withUnsafeBytes(of: UInt16(4).littleEndian) { Data($0) })       // Block align = channels * bits/8 (2 bytes)
        header.append(withUnsafeBytes(of: UInt16(16).littleEndian) { Data($0) })      // Bits per sample (2 bytes)
        
        // data sub-chunk (8 bytes header + data)
        header.append("data".data(using: .ascii)!)                                    // "data" (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(dataLength).littleEndian) { Data($0) }) // Data length (4 bytes)
        
        logger.info("ğŸ“‹ WAVå¤´åˆ›å»º: æ€»å¤§å°=\(header.count + dataLength), å¤´=\(header.count)å­—èŠ‚, PCM=\(dataLength)å­—èŠ‚")
        
        return header
    }
    
    private func notifyConvertedAudioFileCompleted(fileURL: URL) {
        let notification: [String: Any] = [
            "event": "converted_audio_file_completed",
            "fileName": fileURL.lastPathComponent,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("converted_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            logger.info("ğŸ“¡ å·²é€šçŸ¥è½¬æ¢åéŸ³é¢‘æ–‡ä»¶å®Œæˆ")
        } catch {
            logger.error("âŒ å†™å…¥è½¬æ¢åéŸ³é¢‘é€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
}

extension RealtimeAudioStreamManager: SFSpeechRecognizerDelegate {
    nonisolated public func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        Task { @MainActor in
            logger.info("ğŸ¤ è¯­éŸ³è¯†åˆ«å™¨å¯ç”¨æ€§å˜åŒ–: \(available)")
            if !available && isProcessing {
                stopRecognition()
            }
        }
    }
}