import Foundation
import AVFoundation
import os.log
import SocketIO
import Combine

@MainActor
public class RealtimeAudioStreamManager: NSObject, ObservableObject {
    
    @Published public var isProcessing: Bool = false
    @Published public var recognizedText: String = ""
    @Published public var errorMessage: String = ""
    
    // MARK: - Private Properties for text management
    private var previousText: String = ""
    private var shouldPreserveText: Bool = false
    private var currentPartialText: String = ""
    private var finalTextSegments: [String] = []
    
    // Socket.IO Properties
    private var manager: SocketManager?
    private var socket: SocketIOClient?
    private let serverURL = "https://api-test.pleaseprof.app"
    private let namespace = "/transcribe" // Same namespace as SpeechRecognitionManager but separate connection
    
    // Audio Processing Properties
    private var sampleRate: Double = 16000
    private var languageCode: String = "zh-CN"
    private var audioEngine = AVAudioEngine()
    private var audioSession = AVAudioSession.sharedInstance()
    @Published public var isConnected: Bool = false
    private var isRecording: Bool = false
    private let logger = Logger(subsystem: "dev.tuist2.Siri", category: "RealtimeAudio")
    private let appGroupID = "group.dev.tuist2.Siri"
    
    private var darwinNotificationCenter: CFNotificationCenter?
    private var audioBufferQueue = [CMSampleBuffer]()
    
    // ç”¨äºä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®è¿›è¡ŒéªŒè¯
    private var audioWriter: AVAssetWriter?
    private var audioWriterInput: AVAssetWriterInput?
    private var convertedAudioFileURL: URL?
    private var processingQueue = DispatchQueue(label: "realtime.audio.processing", qos: .userInitiated)
    private var hasLoggedFormat = false
    
    // ä¿å­˜éŸ³é¢‘åˆ°m4aæ–‡ä»¶ - å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„æ–¹å¼
    private var m4aAudioWriter: AVAssetWriter?
    private var m4aAudioWriterInput: AVAssetWriterInput?
    private var currentM4AFileURL: URL?
    private var m4aStartTime: CMTime?
    
    public override init() {
        super.init()
        // è®¾ç½®éŸ³é¢‘ä¼šè¯ç¡®ä¿æ‰¬å£°å™¨è¾“å‡º
        setupAudioSession()
        setupSocketConnection()
        setupDarwinNotifications()
    }
    
    // MARK: - Socket.IO Setup
    private func setupSocketConnection() {
        guard let url = URL(string: serverURL) else {
            errorMessage = "Invalid server URL"
            logger.error("âŒ Invalid server URL: \(self.serverURL)")
            return
        }
        
        logger.info("ğŸ”§ [RealtimeManager] Setting up Socket.IO connection to \(self.serverURL) with namespace \(self.namespace)")
        
        manager = SocketManager(socketURL: url, config: [
            .log(true),
            .compress,
            .forceWebsockets(true),
            .forcePolling(false),
            .reconnects(true),
            .reconnectAttempts(3),
            .reconnectWait(1),
            .reconnectWaitMax(5)
        ])
        
        socket = manager?.socket(forNamespace: namespace)
        setupSocketHandlers()
        
        logger.info("âœ… Socket.IO client configured for realtime transcription")
    }
    
    private func setupSocketHandlers() {
        logger.info("ğŸ”Œ Setting up Socket.IO handlers for realtime transcription...")
        
        socket?.on(clientEvent: .connect) { [weak self] data, ack in
            DispatchQueue.main.async {
                self?.isConnected = true
                self?.logger.info("âœ… [RealtimeManager] Connected to transcription service")
                self?.errorMessage = ""
            }
        }
        
        socket?.on("user-assigned") { [weak self] data, ack in
            guard let userInfo = data.first as? [String: Any],
                  let userId = userInfo["userId"] as? String else { return }
            DispatchQueue.main.async {
                self?.logger.info("ğŸ“‹ [RealtimeManager] User ID assigned: \(userId)")
            }
        }
        
        socket?.on(clientEvent: .disconnect) { [weak self] data, ack in
            DispatchQueue.main.async {
                self?.isConnected = false
                self?.logger.info("âŒ [RealtimeManager] Disconnected from service: \(data)")
                self?.errorMessage = "Disconnected from server"
            }
        }
        
        socket?.on(clientEvent: .error) { [weak self] data, ack in
            DispatchQueue.main.async {
                self?.logger.error("ğŸ”´ [RealtimeManager] Socket error: \(data)")
                self?.errorMessage = "Socket error: \(data)"
            }
        }
        
        socket?.on("connect_error") { [weak self] data, ack in
            DispatchQueue.main.async {
                self?.logger.error("ğŸ”´ [RealtimeManager] Connect error: \(data)")
                self?.errorMessage = "Failed to connect: \(data)"
            }
        }
        
        socket?.on("transcription-started") { [weak self] data, ack in
            DispatchQueue.main.async {
                self?.logger.info("ğŸ™ï¸ [RealtimeManager] Transcription started")
            }
        }
        
        socket?.on("transcription-result") { [weak self] data, ack in
            self?.logger.info("ğŸ“ [RealtimeManager] Received transcription result: \(data)")
            
            guard let resultData = data.first as? [String: Any] else {
                self?.logger.error("âŒ Invalid transcription result format: \(data)")
                return
            }
            
            guard let result = resultData["result"] as? [String: Any] else {
                self?.logger.error("âŒ Missing result field in: \(resultData)")
                return
            }
            
            guard let transcript = result["transcript"] as? String else {
                self?.logger.error("âŒ Missing transcript field in: \(result)")
                return
            }
            
            let isPartial = result["isPartial"] as? Bool ?? false
            
            DispatchQueue.main.async {
                self?.handleTranscriptionResult(transcript: transcript, isPartial: isPartial)
            }
        }
        
        socket?.on("transcription-error") { [weak self] data, ack in
            guard let errorData = data.first as? [String: Any],
                  let error = errorData["error"] as? String else { return }
            DispatchQueue.main.async {
                self?.errorMessage = error
                self?.logger.error("âŒ [RealtimeManager] Transcription error: \(error)")
            }
        }
    }
    
    // MARK: - Connection Management
    private func connectToServerAndStartRecording() {
        logger.info("ğŸ”Œ Attempting to connect to realtime transcription server...")
        
        if isConnected {
            logger.info("âœ… Already connected to realtime server")
            startRealtimeRecording()
            return
        }
        
        logger.info("ğŸ”Œ Initiating Socket.IO connection to \(self.serverURL)\(self.namespace)...")
        socket?.connect()
        
        waitForConnection(timeout: 10.0) {
            self.startRealtimeRecording()
        }
    }
    
    private func waitForConnection(timeout: TimeInterval, completion: @escaping () -> Void) {
        let startTime = Date()
        
        func checkConnection() {
            if isConnected {
                logger.info("âœ… Connected successfully to realtime service!")
                completion()
                return
            }
            
            let elapsed = Date().timeIntervalSince(startTime)
            if elapsed > timeout {
                logger.error("â° Connection timeout after \(timeout)s")
                errorMessage = "Failed to connect to realtime transcription server"
                completion()
                return
            }
            
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                checkConnection()
            }
        }
        
        checkConnection()
    }
    
    private func startRealtimeRecording() {
        logger.info("ğŸ™ï¸ Starting realtime recording...")
        
        guard !isRecording else { return }
        
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.mixWithOthers, .allowBluetooth, .defaultToSpeaker])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            try audioSession.overrideOutputAudioPort(.speaker)
            
            // Start M4A recording for audio file saving (same as original functionality)
            startM4ARecording()
            
            let config: [String: Any] = [
                "languageCode": languageCode,
                "sampleRateHertz": Int(sampleRate),
                "mediaEncoding": "pcm",
                "enableSpeakerDiarization": false,
                "maxSpeakerLabels": 2
            ]
            
            socket?.emit("start-transcription", config)
            logger.info("ğŸ“¤ Sent realtime transcription config: \(config)")
            
            startAudioCapture()
            
            isRecording = true
            isProcessing = true
            
            if shouldPreserveText {
                logger.info("ğŸ”’ Preserving previous text: '\(self.previousText)'")
            } else {
                recognizedText = ""
                previousText = ""
                currentPartialText = ""
                finalTextSegments = []
            }
            errorMessage = ""
            
        } catch {
            errorMessage = "Failed to start realtime recording: \(error.localizedDescription)"
            logger.error("âŒ Failed to start realtime recording: \(error.localizedDescription)")
        }
    }
    
    private func startAudioCapture() {
        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)
        
        guard let outputFormat = AVAudioFormat(
            commonFormat: .pcmFormatInt16,
            sampleRate: sampleRate,
            channels: 1,
            interleaved: false
        ) else {
            errorMessage = "Failed to create audio format"
            return
        }
        
        guard let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
            errorMessage = "Failed to create audio converter"
            return
        }
        
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { [weak self] buffer, time in
            self?.processRealtimeAudioBuffer(buffer, converter: converter, outputFormat: outputFormat)
        }
        
        audioEngine.prepare()
        do {
            try audioEngine.start()
            logger.info("ğŸ™ï¸ Realtime audio engine started")
        } catch {
            errorMessage = "Failed to start audio engine: \(error.localizedDescription)"
            logger.error("âŒ Failed to start realtime audio engine: \(error.localizedDescription)")
        }
    }
    
    private func processRealtimeAudioBuffer(_ inputBuffer: AVAudioPCMBuffer, converter: AVAudioConverter, outputFormat: AVAudioFormat) {
        guard isRecording else { return }
        
        let inputFrameCount = inputBuffer.frameLength
        let outputFrameCapacity = AVAudioFrameCount(Double(inputFrameCount) * (sampleRate / inputBuffer.format.sampleRate))
        
        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: outputFrameCapacity) else {
            return
        }
        
        var error: NSError?
        let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
            outStatus.pointee = .haveData
            return inputBuffer
        }
        
        converter.convert(to: outputBuffer, error: &error, withInputFrom: inputBlock)
        
        if let error = error {
            logger.error("âŒ Realtime audio conversion error: \(error.localizedDescription)")
            return
        }
        
        if let pcmData = pcmDataFromBuffer(outputBuffer) {
            sendRealtimeAudioData(pcmData)
        }
    }
    
    private func pcmDataFromBuffer(_ buffer: AVAudioPCMBuffer) -> Data? {
        let frameCount = Int(buffer.frameLength)
        guard frameCount > 0,
              let int16ChannelData = buffer.int16ChannelData else {
            return nil
        }
        
        let channelData = int16ChannelData[0]
        let dataSize = frameCount * MemoryLayout<Int16>.size
        
        let data = Data(bytes: channelData, count: dataSize)
        
        var processedData = Data()
        data.withUnsafeBytes { (bytes: UnsafeRawBufferPointer) in
            guard let int16Pointer = bytes.bindMemory(to: Int16.self).baseAddress else { return }
            
            var maxAmplitude: Int16 = 0
            for i in 0..<frameCount {
                maxAmplitude = max(maxAmplitude, abs(int16Pointer[i]))
            }
            
            let noiseThreshold: Int16 = 328
            let amplificationFactor = maxAmplitude > 0 ? min(Double(Int16.max) * 0.8 / Double(maxAmplitude), 3.0) : 1.0
            
            for i in 0..<frameCount {
                var sample = int16Pointer[i]
                
                if abs(sample) < noiseThreshold {
                    sample = Int16(Double(sample) * 0.1)
                } else {
                    sample = Int16(Double(sample) * amplificationFactor)
                }
                
                sample = max(Int16.min, min(Int16.max, sample))
                
                withUnsafeBytes(of: sample) { sampleBytes in
                    processedData.append(contentsOf: sampleBytes)
                }
            }
        }
        
        return processedData
    }
    
    private func sendRealtimeAudioData(_ data: Data) {
        guard isConnected else {
            logger.warning("âš ï¸ Not connected to realtime server, skipping audio send")
            return
        }
        
        let base64String = data.base64EncodedString()
        let audioData: [String: Any] = ["audio": base64String]
        
        socket?.emit("audio-data", audioData)
        
        if Int.random(in: 0..<100) < 5 {
            logger.debug("ğŸ“¤ Sent realtime audio data: \(data.count) bytes")
        }
    }
    
    private func stopAudioCapture() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        isRecording = false
        
        // Stop M4A recording when stopping audio capture
        stopM4ARecording()
        
        if !currentPartialText.isEmpty {
            finalTextSegments.append(currentPartialText)
            currentPartialText = ""
            logger.info("ğŸ“ Finalized pending partial text")
        }
        
        do {
            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
            logger.info("âœ… Audio session reset")
        } catch {
            logger.error("âš ï¸ Failed to reset audio session: \(error.localizedDescription)")
        }
    }
    
    // MARK: - Transcription Result Handling
    private func handleTranscriptionResult(transcript: String, isPartial: Bool) {
        logger.info("ğŸ™ï¸ Realtime transcription result: '\(transcript)' (partial: \(isPartial))")
        
        if !transcript.isEmpty {
            if isPartial {
                currentPartialText = transcript
                logger.info("ğŸ”„ Partial result updated: '\(self.currentPartialText)'")
            } else {
                finalTextSegments.append(transcript)
                currentPartialText = ""
                logger.info("âœ… Final result added. Total segments: \(self.finalTextSegments.count)")
            }
            
            var completeText = self.finalTextSegments.joined(separator: " ")
            if !self.currentPartialText.isEmpty {
                if !completeText.isEmpty {
                    completeText += " " + self.currentPartialText
                } else {
                    completeText = self.currentPartialText
                }
            }
            
            if shouldPreserveText, !previousText.isEmpty {
                recognizedText = previousText + "\n" + completeText
            } else {
                recognizedText = completeText
            }
            
            logger.info("ğŸ“ Updated realtime text: '\(self.recognizedText)'")
        }
    }
    
    public func startMonitoring() {
        logger.info("ğŸš€ Starting realtime audio monitoring with Socket.IO")
        
        audioSession.requestRecordPermission { [weak self] granted in
            DispatchQueue.main.async {
                if granted {
                    self?.logger.info("âœ… Microphone permission granted for realtime monitoring")
                    self?.connectToServerAndStartRecording()
                } else {
                    self?.errorMessage = "Microphone permission denied"
                }
            }
        }
    }
    
    public func stopMonitoring() {
        logger.info("ğŸ›‘ åœæ­¢å®æ—¶éŸ³é¢‘æµç›‘æ§")
        stopRecognition()
        
        if isRecording {
            stopAudioCapture()
        }
        
        socket?.emit("stop-transcription")
    }
    
    // MARK: - Text Preservation Methods
    private var textPreservationRequested: Bool = false  // è·Ÿè¸ªæ˜¯å¦è¯·æ±‚äº†æ–‡å­—ä¿ç•™
    
    public func setTextPreservationMode(_ preserve: Bool) {
        textPreservationRequested = preserve
        shouldPreserveText = preserve
        if preserve {
            // ä¿å­˜å½“å‰æ–‡å­—
            previousText = recognizedText
            logger.info("ğŸ”’ å¯ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œä¿å­˜æ–‡å­—: '\(self.previousText)'")
        } else {
            // æ¸…é™¤ä¿å­˜çš„æ–‡å­—
            previousText = ""
            textPreservationRequested = false
            logger.info("ğŸ”“ ç¦ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼")
        }
    }
    
    private func setupDarwinNotifications() {
        darwinNotificationCenter = CFNotificationCenterGetDarwinNotifyCenter()
        
        let notificationName = "dev.tuist2.Siri.audiodata" as CFString
        let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque())
        
        CFNotificationCenterAddObserver(
            darwinNotificationCenter,
            observer,
            { (center, observer, name, object, userInfo) in
                guard let observer = observer else { return }
                let manager = Unmanaged<RealtimeAudioStreamManager>.fromOpaque(observer).takeUnretainedValue()
                Task { @MainActor in
                    manager.handleAudioDataNotification()
                }
            },
            notificationName,
            nil,
            .deliverImmediately
        )
        
        logger.info("ğŸ“¡ Darwiné€šçŸ¥ç›‘å¬å·²è®¾ç½®")
    }
    
    private func handleAudioDataNotification() {
        logger.debug("ğŸ“± æ”¶åˆ°éŸ³é¢‘æ•°æ®é€šçŸ¥")
        readAudioDataFromSharedMemory()
    }
    
    private func readAudioDataFromSharedMemory() {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let audioDataURL = containerURL.appendingPathComponent("realtime_audio_buffer.data")
        let formatURL = containerURL.appendingPathComponent("realtime_audio_format.json")
        
        guard FileManager.default.fileExists(atPath: audioDataURL.path),
              FileManager.default.fileExists(atPath: formatURL.path),
              let audioData = try? Data(contentsOf: audioDataURL),
              let formatData = try? Data(contentsOf: formatURL),
              let formatInfo = try? JSONSerialization.jsonObject(with: formatData) as? [String: Any] else {
            return
        }
        
        processingQueue.async { [weak self] in
            Task { @MainActor in
                self?.processAudioData(audioData, formatInfo: formatInfo)
            }
        }
    }
    
    private func processAudioData(_ data: Data, formatInfo: [String: Any]) {
        // é¦–å…ˆï¼Œä½¿ç”¨åŸå§‹æ•°æ®é‡å»ºCMSampleBufferå¹¶ä¿å­˜åˆ°m4aæ–‡ä»¶ï¼ˆå®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerï¼‰
        if let sampleBuffer = createSampleBufferFromData(data, formatInfo: formatInfo) {
            saveOriginalAudioToFile(sampleBuffer)
            
            // ä½¿ç”¨é‡å»ºçš„CMSampleBufferè¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼ˆæ•°æ®æºå·²éªŒè¯æ­£å¸¸ï¼‰
            if isProcessing {
                performSpeechRecognitionWithSampleBuffer(sampleBuffer)
                return
            }
        }
        
        guard isProcessing else {
            return
        }
        
        // ä»æ ¼å¼ä¿¡æ¯ä¸­è·å–éŸ³é¢‘å‚æ•°
        guard let sampleRate = formatInfo["sampleRate"] as? Double,
              let channels = formatInfo["channels"] as? UInt32,
              let formatID = formatInfo["formatID"] as? UInt32,
              let bitsPerChannel = formatInfo["bitsPerChannel"] as? UInt32 else {
            logger.error("âŒ æ— æ³•è§£æéŸ³é¢‘æ ¼å¼ä¿¡æ¯")
            return
        }
        
        // åªåœ¨é¦–æ¬¡è¯†åˆ«æ ¼å¼æ—¶è®°å½•
        if !hasLoggedFormat && (formatID == kAudioFormatLinearPCM || formatID == 1819304813) {
            logger.info("ğŸµ PCMæ ¼å¼: \(sampleRate)Hz, \(channels)å£°é“, \(bitsPerChannel)ä½")
            hasLoggedFormat = true
        }
        
        // åˆ›å»ºåˆé€‚çš„éŸ³é¢‘æ ¼å¼
        var audioFormat: AVAudioFormat?
        
        // æ ¹æ®å®é™…æ ¼å¼åˆ›å»ºAVAudioFormat - ä½¿ç”¨ä¸æˆåŠŸWAVè½¬æ¢ç›¸åŒçš„æ ¼å¼
        // kAudioFormatLinearPCM = 1819304813 ('lpcm') æ˜¯çº¿æ€§PCMæ ¼å¼çš„æ ‡è¯†ç¬¦
        if formatID == kAudioFormatLinearPCM || formatID == 1819304813 {
            // PCMæ ¼å¼ - å‚è€ƒæˆåŠŸçš„WAVè½¬æ¢æ ¼å¼
            if bitsPerChannel == 16 {
                // 16ä½æ•´æ•° - ä½¿ç”¨ä¸WAVè½¬æ¢ç›¸åŒçš„æ ¼å¼å‚æ•°
                // å‚è€ƒAudioFileManagerä¸­æˆåŠŸçš„è½¬æ¢æ ¼å¼ï¼š
                // mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagIsPacked | kAudioFormatFlagsNativeEndian
                var asbd = AudioStreamBasicDescription()
                asbd.mSampleRate = sampleRate
                asbd.mFormatID = kAudioFormatLinearPCM
                asbd.mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagIsPacked | kAudioFormatFlagsNativeEndian
                asbd.mBitsPerChannel = 16
                asbd.mChannelsPerFrame = UInt32(channels)
                asbd.mBytesPerFrame = asbd.mChannelsPerFrame * 2
                asbd.mFramesPerPacket = 1
                asbd.mBytesPerPacket = asbd.mBytesPerFrame
                
                audioFormat = AVAudioFormat(streamDescription: &asbd)
                logger.info("ğŸµ ä½¿ç”¨WAVå…¼å®¹æ ¼å¼: 16-bit signed integer, native endian, \(channels)å£°é“")
            } else if bitsPerChannel == 32 {
                // 32ä½æµ®ç‚¹ - ä¿æŒåŸæœ‰æ ¼å¼
                audioFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: AVAudioChannelCount(channels), interleaved: false)
                logger.info("ğŸµ ä½¿ç”¨32ä½æµ®ç‚¹æ ¼å¼")
            }
        }
        
        guard let format = audioFormat else {
            logger.error("âŒ ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: formatID=\(formatID), bitsPerChannel=\(bitsPerChannel)")
            return
        }
        
        // è®¡ç®—å¸§æ•°é‡
        let bytesPerSample = bitsPerChannel / 8
        let frameCount = AVAudioFrameCount(data.count / (Int(bytesPerSample) * Int(channels)))
        
        guard frameCount > 0 else {
            return
        }
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
            return
        }
        
        audioBuffer.frameLength = frameCount
        
        // å¤åˆ¶éŸ³é¢‘æ•°æ® - 16ä½æ ¼å¼ä½¿ç”¨äº¤é”™æ•°æ®ï¼ˆä¸WAVè½¬æ¢æ ¼å¼ä¸€è‡´ï¼‰
        data.withUnsafeBytes { (rawBytes: UnsafeRawBufferPointer) in
            if bitsPerChannel == 16 {
                // 16ä½æ•´æ•°æ•°æ® - ä½¿ç”¨äº¤é”™æ ¼å¼ï¼ˆä¸æˆåŠŸçš„WAVè½¬æ¢ä¸€è‡´ï¼‰
                guard let int16Pointer = rawBytes.bindMemory(to: Int16.self).baseAddress else {
                    return
                }
                
                // å¯¹äºäº¤é”™æ ¼å¼ï¼Œç›´æ¥å¤åˆ¶åŸå§‹æ•°æ®
                if let audioDataPointer = audioBuffer.audioBufferList.pointee.mBuffers.mData {
                    let sampleCount = Int(frameCount) * Int(channels)
                    let audioInt16Pointer = audioDataPointer.bindMemory(to: Int16.self, capacity: sampleCount)
                    audioInt16Pointer.initialize(from: int16Pointer, count: sampleCount)
                    
                    // éªŒè¯æ•°æ®
                    let firstSample = int16Pointer[0]
                    let secondSample = channels > 1 ? int16Pointer[1] : firstSample
                    logger.info("ğŸ” äº¤é”™æ ¼å¼å¤åˆ¶: é¦–æ ·æœ¬=\(firstSample), æ¬¡æ ·æœ¬=\(secondSample), æ€»æ ·æœ¬æ•°=\(sampleCount)")
                }
                
            } else if bitsPerChannel == 32 {
                // 32ä½æµ®ç‚¹æ•°æ® - ä»äº¤é”™è½¬ä¸ºéäº¤é”™
                guard let floatPointer = rawBytes.bindMemory(to: Float.self).baseAddress,
                      let channelData = audioBuffer.floatChannelData else {
                    return
                }
                
                if channels == 2 {
                    // ç«‹ä½“å£°ï¼šåˆ†ç¦»äº¤é”™æ•°æ®
                    let leftChannel = channelData[0]
                    let rightChannel = channelData[1]
                    
                    for frame in 0..<Int(frameCount) {
                        let interleavedIndex = frame * 2
                        leftChannel[frame] = floatPointer[interleavedIndex]
                        rightChannel[frame] = floatPointer[interleavedIndex + 1]
                    }
                } else {
                    // å•å£°é“ï¼šç›´æ¥å¤åˆ¶
                    let channel = channelData[0]
                    channel.initialize(from: floatPointer, count: Int(frameCount))
                }
            }
        }
        
        // æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥ä¿å­˜audioBufferï¼Œå› ä¸ºå®ƒæ˜¯è½¬æ¢åçš„æ ¼å¼
        // æˆ‘ä»¬éœ€è¦ä¿å­˜åŸå§‹çš„CMSampleBufferï¼Œä½†è¿™é‡Œåªæœ‰è½¬æ¢åçš„AVAudioPCMBuffer
        // æ‰€ä»¥m4aä¿å­˜éœ€è¦åœ¨processAudioDataä¸­è¿›è¡Œï¼Œä½¿ç”¨åŸå§‹æ•°æ®
        
        // ä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®ç”¨äºéªŒè¯
        saveConvertedAudioBuffer(audioBuffer)  // é‡æ–°å¯ç”¨ï¼Œæµ‹è¯•æ–°çš„äº¤é”™æ ¼å¼
        
        // Note: Using Socket.IO instead of traditional recognition request
        // The audio data is processed through Socket.IO connection
    }
    
    private func calculateAudioLevel(from audioBuffer: AVAudioPCMBuffer) -> Double {
        // å¯¹äºäº¤é”™æ ¼å¼ï¼Œä½¿ç”¨audioBufferListè®¿é—®æ•°æ®
        guard let audioDataPointer = audioBuffer.audioBufferList.pointee.mBuffers.mData else {
            return 0.0
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(audioBuffer.format.channelCount)
        let sampleCount = frameCount * channels
        
        let int16Pointer = audioDataPointer.bindMemory(to: Int16.self, capacity: sampleCount)
        
        var sum: Double = 0.0
        
        for i in 0..<sampleCount {
            let sample = Double(int16Pointer[i]) / 32768.0 // å½’ä¸€åŒ–åˆ° -1.0 åˆ° 1.0
            sum += sample * sample
        }
        
        let rms = sqrt(sum / Double(sampleCount))
        return rms
    }
    
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            // ä½¿ç”¨ playback æ¨¡å¼ï¼Œç¡®ä¿ä»æ‰¬å£°å™¨è¾“å‡ºï¼ŒåŒæ—¶æ”¯æŒä¸å…¶ä»–éŸ³é¢‘æ··åˆ
            // ç§»é™¤ .duckOthers é€‰é¡¹ï¼Œé¿å…å¹²æ‰°å…¶ä»–éŸ³é¢‘æ’­æ”¾
            try audioSession.setCategory(.playback, mode: .default, options: [.mixWithOthers])
            // å¼ºåˆ¶è®¾ç½®éŸ³é¢‘è·¯ç”±åˆ°æ‰¬å£°å™¨
            try audioSession.overrideOutputAudioPort(.speaker)
            logger.info("ğŸµ éŸ³é¢‘ä¼šè¯è®¾ç½®æˆåŠŸ (playback + default)")
        } catch {
            logger.error("âŒ éŸ³é¢‘ä¼šè¯è®¾ç½®å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func startRecognition() {
        logger.info("ğŸš€ å¼€å§‹åŸºäºSocket.IOçš„è¯­éŸ³è¯†åˆ«")
        
        guard !isProcessing else {
            return
        }
        
        // å¦‚æœä¹‹å‰è¯·æ±‚äº†æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œé‡æ–°å¯ç”¨
        if textPreservationRequested {
            shouldPreserveText = true
            logger.info("ğŸ”„ æ¢å¤æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œä¹‹å‰ä¿å­˜çš„æ–‡å­—: '\(self.previousText)'")
        }
        
        // ç¡®ä¿éŸ³é¢‘ä»æ‰¬å£°å™¨è¾“å‡º
        do {
            try AVAudioSession.sharedInstance().overrideOutputAudioPort(.speaker)
            logger.info("ğŸ”Š å¼ºåˆ¶éŸ³é¢‘è·¯ç”±åˆ°æ‰¬å£°å™¨")
        } catch {
            logger.error("âŒ è®¾ç½®æ‰¬å£°å™¨è¾“å‡ºå¤±è´¥: \(error.localizedDescription)")
        }
        
        isProcessing = true
        hasLoggedFormat = false
        
        // é‡ç½®m4aå½•åˆ¶çŠ¶æ€
        m4aStartTime = nil
        
        // å¼€å§‹m4aæ–‡ä»¶å½•åˆ¶ - å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandler
        startM4ARecording()
        
        // å¼€å§‹å½•åˆ¶è½¬æ¢åçš„éŸ³é¢‘ç”¨äºéªŒè¯ - åªä¿å­˜WAVç”¨äºéªŒè¯éŸ³é¢‘è´¨é‡
        startConvertedAudioRecording()  // ä¿å­˜ç”¨äºè¯­éŸ³è¯†åˆ«çš„éŸ³é¢‘æ•°æ®ä¸ºWAVæ ¼å¼
        
        // ä½¿ç”¨Socket.IOè¿›è¡Œå®æ—¶è¯­éŸ³è¯†åˆ«ï¼Œè€Œä¸æ˜¯å¤„ç†å·²å½•åˆ¶çš„éŸ³é¢‘æ•°æ®
        connectToServerAndStartRecording()
        
        logger.info("âœ… åŸºäºSocket.IOçš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡å·²å¯åŠ¨")
    }
    
    private func stopRecognition() {
        logger.info("ğŸ›‘ åœæ­¢Socket.IOè¯­éŸ³è¯†åˆ«")
        
        // åœæ­¢Socket.IOç›¸å…³çš„å½•éŸ³
        if isRecording {
            stopAudioCapture()
        }
        
        // å‘é€åœæ­¢ä¿¡å·åˆ°æœåŠ¡å™¨
        socket?.emit("stop-transcription")
        
        isProcessing = false
        
        // åœæ­¢è¯†åˆ«æ—¶æš‚æ—¶ç¦ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼ï¼Œé˜²æ­¢å»¶è¿Ÿå›è°ƒå¯¼è‡´æ–‡å­—é‡å¤
        if shouldPreserveText {
            logger.info("â¸ï¸ åœæ­¢è¯†åˆ«æ—¶æš‚æ—¶ç¦ç”¨æ–‡å­—ä¿ç•™æ¨¡å¼")
            shouldPreserveText = false
        }
        
        // åœæ­¢m4aæ–‡ä»¶å½•åˆ¶ - æ¨¡ä»¿ScreenBroadcastHandlerçš„stopAudioRecording
        stopM4ARecording()
        
        // åœæ­¢å½•åˆ¶è½¬æ¢åçš„éŸ³é¢‘
        // stopConvertedAudioRecording()  // æš‚æ—¶æ³¨é‡Šæ‰ï¼Œåªæµ‹è¯•M4Aåˆ°WAVè½¬æ¢
        
        logger.info("âœ… Socket.IOè¯­éŸ³è¯†åˆ«å·²åœæ­¢")
    }
    
    deinit {
        let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque())
        CFNotificationCenterRemoveObserver(
            darwinNotificationCenter,
            observer,
            CFNotificationName("dev.tuist2.Siri.audiodata" as CFString),
            nil
        )
        socket?.disconnect()
    }
    
    // MARK: - Converted Audio Recording for Verification
    
    private func startConvertedAudioRecording() {
        // ç®€åŒ–ï¼šä¸å†åˆ›å»ºç‹¬ç«‹çš„éŸ³é¢‘æ–‡ä»¶ï¼Œè€Œæ˜¯åœ¨ç»“æŸæ—¶ä½¿ç”¨M4Aæ–‡ä»¶è½¬æ¢
        logger.info("ğŸ™ï¸ å¼€å§‹è¯­éŸ³è¯†åˆ«ä¼šè¯ - å°†åœ¨å½•åˆ¶ç»“æŸæ—¶ä»M4Aæ–‡ä»¶åˆ›å»ºéªŒè¯WAV")
    }
    
    private func saveConvertedAudioBuffer(_ audioBuffer: AVAudioPCMBuffer) {
        // ç®€åŒ–ï¼šä¸å†ä¿å­˜éŸ³é¢‘æ•°æ®ï¼Œä¸“æ³¨äºè¯­éŸ³è¯†åˆ«åŠŸèƒ½
        // éªŒè¯éŸ³é¢‘è´¨é‡çš„WAVæ–‡ä»¶å°†åœ¨å½•åˆ¶ç»“æŸæ—¶ä»M4Aæ–‡ä»¶ç”Ÿæˆ
        
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(audioBuffer.format.channelCount)
        logger.debug("ğŸ”„ å¤„ç†éŸ³é¢‘ç¼“å†²åŒº: \(frameCount)å¸§, \(channels)å£°é“")
    }
    
    private func stopConvertedAudioRecording() {
        // ä½¿ç”¨æœ€æ–°çš„M4Aæ–‡ä»¶è½¬æ¢ä¸ºWAVç”¨äºéªŒè¯è¯­éŸ³è¯†åˆ«éŸ³é¢‘è´¨é‡
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        
        // æŸ¥æ‰¾æœ€æ–°çš„M4Aæ–‡ä»¶ï¼ˆä¸M4Aè½¬WAVè½¬æ¢ä½¿ç”¨ç›¸åŒæ•°æ®æºï¼‰
        do {
            let files = try FileManager.default.contentsOfDirectory(at: audioDirectory, includingPropertiesForKeys: [.creationDateKey])
            
            let m4aFiles = files.filter { $0.pathExtension == "m4a" && $0.lastPathComponent.hasPrefix("SystemAudio_") }
                .sorted { file1, file2 in
                    let date1 = (try? file1.resourceValues(forKeys: [.creationDateKey]).creationDate) ?? Date.distantPast
                    let date2 = (try? file2.resourceValues(forKeys: [.creationDateKey]).creationDate) ?? Date.distantPast
                    return date1 > date2
                }
            
            guard let latestM4A = m4aFiles.first else {
                logger.info("âš ï¸ æ²¡æœ‰æ‰¾åˆ°M4Aæ–‡ä»¶ç”¨äºéªŒè¯")
                return
            }
            
            // åˆ›å»ºéªŒè¯WAVæ–‡ä»¶å
            let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium).replacingOccurrences(of: ":", with: "-")
            let fileName = "RealtimeRecognition_\(timestamp).wav"
            let finalURL = audioDirectory.appendingPathComponent(fileName)
            
            // ä½¿ç”¨ä¸æ­£å¸¸è½¬æ¢ç›¸åŒçš„æ–¹æ³•è½¬æ¢M4Aåˆ°WAV
            let audioFileManager = AudioFileManager()
            if let wavURL = audioFileManager.convertM4AToWAV(m4aURL: latestM4A) {
                // é‡å‘½åä¸ºéªŒè¯æ–‡ä»¶
                try FileManager.default.moveItem(at: wavURL, to: finalURL)
                logger.info("âœ… è¯­éŸ³è¯†åˆ«éªŒè¯WAVæ–‡ä»¶å·²åˆ›å»º: \(fileName)")
                notifyConvertedAudioFileCompleted(fileURL: finalURL)
            } else {
                logger.error("âŒ M4Aè½¬WAVå¤±è´¥")
            }
            
        } catch {
            logger.error("âŒ æŸ¥æ‰¾M4Aæ–‡ä»¶å¤±è´¥: \(error.localizedDescription)")
        }
        
        // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        let tempDataURL = audioDirectory.appendingPathComponent("realtime_recognition_temp.pcm")
        try? FileManager.default.removeItem(at: tempDataURL)
        
        audioWriter = nil
        audioWriterInput = nil
        convertedAudioFileURL = nil
    }
    
    private func convertPCMToWAV(inputURL: URL, outputURL: URL) {
        do {
            let pcmData = try Data(contentsOf: inputURL)
            guard pcmData.count > 0 else {
                logger.error("âŒ PCMæ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºWAVæ–‡ä»¶")
                return
            }
            
            let wavData = createWAVHeader(dataLength: pcmData.count) + pcmData
            try wavData.write(to: outputURL)
            logger.info("ğŸ“ WAVæ–‡ä»¶å·²åˆ›å»º: \(outputURL.lastPathComponent), å¤§å°: \(wavData.count) bytes (PCM: \(pcmData.count) bytes)")
        } catch {
            logger.error("âŒ è½¬æ¢PCMä¸ºWAVå¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func createWAVHeader(dataLength: Int) -> Data {
        var header = Data()
        
        // RIFF header (12 bytes)
        header.append("RIFF".data(using: .ascii)!)                                    // "RIFF" (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(36 + dataLength).littleEndian) { Data($0) }) // File size - 8 (4 bytes)
        header.append("WAVE".data(using: .ascii)!)                                    // "WAVE" (4 bytes)
        
        // fmt sub-chunk (24 bytes)
        header.append("fmt ".data(using: .ascii)!)                                    // "fmt " (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(16).littleEndian) { Data($0) })      // fmt chunk size (4 bytes)
        header.append(withUnsafeBytes(of: UInt16(1).littleEndian) { Data($0) })       // PCM = 1 (2 bytes)
        header.append(withUnsafeBytes(of: UInt16(2).littleEndian) { Data($0) })       // 2 channels (2 bytes)
        header.append(withUnsafeBytes(of: UInt32(44100).littleEndian) { Data($0) })   // Sample rate (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(44100 * 2 * 2).littleEndian) { Data($0) }) // Byte rate (4 bytes)
        header.append(withUnsafeBytes(of: UInt16(4).littleEndian) { Data($0) })       // Block align = channels * bits/8 (2 bytes)
        header.append(withUnsafeBytes(of: UInt16(16).littleEndian) { Data($0) })      // Bits per sample (2 bytes)
        
        // data sub-chunk (8 bytes header + data)
        header.append("data".data(using: .ascii)!)                                    // "data" (4 bytes)
        header.append(withUnsafeBytes(of: UInt32(dataLength).littleEndian) { Data($0) }) // Data length (4 bytes)
        
        logger.info("ğŸ“‹ WAVå¤´åˆ›å»º: æ€»å¤§å°=\(header.count + dataLength), å¤´=\(header.count)å­—èŠ‚, PCM=\(dataLength)å­—èŠ‚")
        
        return header
    }
    
    private func notifyConvertedAudioFileCompleted(fileURL: URL) {
        let notification: [String: Any] = [
            "event": "converted_audio_file_completed",
            "fileName": fileURL.lastPathComponent,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("converted_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            logger.info("ğŸ“¡ å·²é€šçŸ¥è½¬æ¢åéŸ³é¢‘æ–‡ä»¶å®Œæˆ")
        } catch {
            logger.error("âŒ å†™å…¥è½¬æ¢åéŸ³é¢‘é€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    // MARK: - M4A Audio Recording
    
    // å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„startAudioRecordingæ–¹æ³•
    private func startM4ARecording() {
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            logger.error("âŒ æ— æ³•è·å–App Groupå®¹å™¨è·¯å¾„")
            return
        }
        
        // åˆ›å»ºéŸ³é¢‘ç›®å½•
        let audioDirectory = containerURL.appendingPathComponent("AudioRecordings")
        if !FileManager.default.fileExists(atPath: audioDirectory.path) {
            do {
                try FileManager.default.createDirectory(at: audioDirectory, withIntermediateDirectories: true, attributes: nil)
            } catch {
                logger.error("âŒ åˆ›å»ºéŸ³é¢‘ç›®å½•å¤±è´¥: \(error.localizedDescription)")
                return
            }
        }
        
        // åˆ›å»ºéŸ³é¢‘æ–‡ä»¶
        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium).replacingOccurrences(of: ":", with: "-")
        let fileName = "RealtimeRecognition_\(timestamp).m4a"
        currentM4AFileURL = audioDirectory.appendingPathComponent(fileName)
        
        guard let audioFileURL = currentM4AFileURL else { return }
        
        // è®¾ç½®éŸ³é¢‘å†™å…¥å™¨ - å®Œå…¨å¤åˆ¶ScreenBroadcastHandlerçš„é…ç½®
        do {
            m4aAudioWriter = try AVAssetWriter(outputURL: audioFileURL, fileType: .m4a)
            
            // é…ç½®éŸ³é¢‘è®¾ç½® - ä¸ScreenBroadcastHandlerå®Œå…¨ä¸€è‡´
            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 44100.0,
                AVNumberOfChannelsKey: 2,
                AVEncoderBitRateKey: 128000
            ]
            
            m4aAudioWriterInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            m4aAudioWriterInput?.expectsMediaDataInRealTime = true
            
            if let input = m4aAudioWriterInput {
                m4aAudioWriter?.add(input)
                m4aAudioWriter?.startWriting()
            }
            
            logger.info("ğŸ™ï¸ å¼€å§‹å½•åˆ¶éŸ³é¢‘: \(fileName)")
            
            // é€šçŸ¥ä¸»ç¨‹åºæ–°æ–‡ä»¶å·²åˆ›å»º
            notifyM4AAudioFileCreated(fileName: fileName, fileURL: audioFileURL)
            
        } catch {
            logger.error("âŒ åˆ›å»ºéŸ³é¢‘å†™å…¥å™¨å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    // å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„saveAudioToFileæ–¹æ³•
    private func saveOriginalAudioToFile(_ sampleBuffer: CMSampleBuffer) {
        guard let writer = m4aAudioWriter,
              let input = m4aAudioWriterInput else {
            logger.error("âŒ m4aAudioWriter æˆ– m4aAudioWriterInput ä¸º nil")
            return
        }
        
        guard writer.status == .writing else {
            logger.error("âŒ AVAssetWriterçŠ¶æ€ä¸æ˜¯writing: \(writer.status.rawValue)")
            return
        }
        
        guard input.isReadyForMoreMediaData else {
            logger.warning("âš ï¸ AVAssetWriterInput ä¸å‡†å¤‡æ¥æ”¶æ›´å¤šæ•°æ®")
            return
        }
        
        // è®¾ç½®å¼€å§‹æ—¶é—´
        if m4aStartTime == nil {
            m4aStartTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)
            logger.info("ğŸµ å‡†å¤‡å¼€å§‹M4Aå½•åˆ¶ä¼šè¯ï¼Œæ—¶é—´æˆ³: \(CMTimeGetSeconds(self.m4aStartTime!))")
            
            // ç¡®ä¿æ—¶é—´æˆ³æœ‰æ•ˆ
            guard CMTIME_IS_VALID(m4aStartTime!) && CMTIME_IS_NUMERIC(m4aStartTime!) else {
                logger.error("âŒ æ— æ•ˆçš„å¼€å§‹æ—¶é—´æˆ³")
                m4aStartTime = nil
                return
            }
            
            writer.startSession(atSourceTime: m4aStartTime!)
            logger.info("âœ… M4Aå½•åˆ¶ä¼šè¯å·²å¼€å§‹")
        }
        
        // å†™å…¥éŸ³é¢‘æ•°æ® - å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandler
        let success = input.append(sampleBuffer)
        if success {
            logger.debug("âœ… æˆåŠŸå†™å…¥éŸ³é¢‘æ•°æ®åˆ°M4Aæ–‡ä»¶")
        } else {
            logger.error("âŒ å†™å…¥éŸ³é¢‘æ•°æ®åˆ°M4Aæ–‡ä»¶å¤±è´¥")
        }
    }
    
    // ä»åŸå§‹éŸ³é¢‘æ•°æ®é‡å»ºCMSampleBuffer
    private func createSampleBufferFromData(_ data: Data, formatInfo: [String: Any]) -> CMSampleBuffer? {
        guard let sampleRate = formatInfo["sampleRate"] as? Double,
              let channels = formatInfo["channels"] as? UInt32,
              let formatID = formatInfo["formatID"] as? UInt32,
              let formatFlags = formatInfo["formatFlags"] as? UInt32,  // å…³é”®ï¼šè¯»å–formatFlags
              let bitsPerChannel = formatInfo["bitsPerChannel"] as? UInt32,
              let bytesPerFrame = formatInfo["bytesPerFrame"] as? UInt32,
              let framesPerPacket = formatInfo["framesPerPacket"] as? UInt32,
              let bytesPerPacket = formatInfo["bytesPerPacket"] as? UInt32 else {
            logger.error("âŒ éŸ³é¢‘æ ¼å¼ä¿¡æ¯ä¸å®Œæ•´: \(formatInfo)")
            return nil
        }
        
        logger.info("ğŸ” é‡å»ºCMSampleBuffer - æ•°æ®å¤§å°: \(data.count)bytes, æ ¼å¼: \(sampleRate)Hz, \(channels)å£°é“, \(bitsPerChannel)ä½, formatID: \(formatID), flags: \(formatFlags)")
        
        // åˆ›å»ºéŸ³é¢‘æµåŸºæœ¬æè¿°
        var asbd = AudioStreamBasicDescription()
        asbd.mSampleRate = sampleRate
        asbd.mFormatID = formatID
        asbd.mChannelsPerFrame = channels
        asbd.mBitsPerChannel = bitsPerChannel
        asbd.mBytesPerFrame = bytesPerFrame
        asbd.mFramesPerPacket = framesPerPacket
        asbd.mBytesPerPacket = bytesPerPacket
        // å…³é”®ï¼šç›´æ¥ä½¿ç”¨ä»æ‰©å±•ç¨‹åºå‘é€çš„åŸå§‹formatFlags
        asbd.mFormatFlags = formatFlags
        
        logger.info("ğŸµ ä½¿ç”¨åŸå§‹éŸ³é¢‘æ ¼å¼æ ‡å¿—: \(formatFlags)")
        
        // åˆ›å»ºéŸ³é¢‘æ ¼å¼æè¿°
        var formatDescription: CMAudioFormatDescription?
        let formatStatus = CMAudioFormatDescriptionCreate(
            allocator: kCFAllocatorDefault,
            asbd: &asbd,
            layoutSize: 0,
            layout: nil,
            magicCookieSize: 0,
            magicCookie: nil,
            extensions: nil,
            formatDescriptionOut: &formatDescription
        )
        
        guard formatStatus == noErr, let audioFormatDescription = formatDescription else {
            logger.error("âŒ åˆ›å»ºéŸ³é¢‘æ ¼å¼æè¿°å¤±è´¥: \(formatStatus)")
            return nil
        }
        
        // åˆ›å»ºCMBlockBuffer - ä½¿ç”¨æ‹·è´æ–¹å¼ç¡®ä¿æ•°æ®å®‰å…¨
        var blockBuffer: CMBlockBuffer?
        let blockBufferStatus = CMBlockBufferCreateWithMemoryBlock(
            allocator: kCFAllocatorDefault,
            memoryBlock: nil,  // è®©ç³»ç»Ÿåˆ†é…å†…å­˜
            blockLength: data.count,
            blockAllocator: kCFAllocatorDefault,
            customBlockSource: nil,
            offsetToData: 0,
            dataLength: data.count,
            flags: 0,
            blockBufferOut: &blockBuffer
        )
        
        // å°†æ•°æ®æ‹·è´åˆ°CMBlockBufferä¸­
        if blockBufferStatus == noErr, let audioBlockBuffer = blockBuffer {
            let copyStatus = data.withUnsafeBytes { dataPtr in
                CMBlockBufferReplaceDataBytes(
                    with: dataPtr.baseAddress!,
                    blockBuffer: audioBlockBuffer,
                    offsetIntoDestination: 0,
                    dataLength: data.count
                )
            }
            
            if copyStatus != noErr {
                logger.error("âŒ æ‹·è´éŸ³é¢‘æ•°æ®åˆ°CMBlockBufferå¤±è´¥: \(copyStatus)")
                return nil
            }
        }
        
        guard blockBufferStatus == noErr, let audioBlockBuffer = blockBuffer else {
            logger.error("âŒ åˆ›å»ºCMBlockBufferå¤±è´¥: \(blockBufferStatus)")
            return nil
        }
        
        // åˆ›å»ºæ—¶é—´æˆ³ä¿¡æ¯
        let frameCount = data.count / Int(bytesPerFrame)
        var sampleTiming = CMSampleTimingInfo()
        sampleTiming.duration = CMTime(value: CMTimeValue(frameCount), timescale: CMTimeScale(sampleRate))
        sampleTiming.presentationTimeStamp = CMTime(value: CMTimeValue(Date().timeIntervalSince1970 * sampleRate), timescale: CMTimeScale(sampleRate))
        sampleTiming.decodeTimeStamp = CMTime.invalid
        
        // åˆ›å»ºCMSampleBuffer
        var sampleBuffer: CMSampleBuffer?
        let sampleBufferStatus = CMSampleBufferCreate(
            allocator: kCFAllocatorDefault,
            dataBuffer: audioBlockBuffer,
            dataReady: true,
            makeDataReadyCallback: nil,
            refcon: nil,
            formatDescription: audioFormatDescription,
            sampleCount: CMItemCount(frameCount),
            sampleTimingEntryCount: 1,
            sampleTimingArray: &sampleTiming,
            sampleSizeEntryCount: 0,
            sampleSizeArray: nil,
            sampleBufferOut: &sampleBuffer
        )
        
        guard sampleBufferStatus == noErr else {
            logger.error("âŒ åˆ›å»ºCMSampleBufferå¤±è´¥: \(sampleBufferStatus)")
            return nil
        }
        
        return sampleBuffer
    }
    
    // MARK: - Speech Recognition with CMSampleBuffer
    
    private func performSpeechRecognitionWithSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
        // Note: This method now processes audio for Socket.IO instead of SFSpeechRecognizer
        logger.info("ğŸ”„ Processing audio buffer for Socket.IO transcription")
        
        // ä» CMSampleBuffer åˆ›å»º AVAudioPCMBuffer ç”¨äºè¯­éŸ³è¯†åˆ«
        guard let audioBuffer = createAudioPCMBufferFromSampleBuffer(sampleBuffer) else {
            logger.warning("âš ï¸ ä»CMSampleBufferåˆ›å»ºAVAudioPCMBufferå¤±è´¥")
            return
        }
        
        // æ£€æŸ¥éŸ³é¢‘æ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯é™éŸ³ï¼‰
        let audioLevel = calculateSimpleAudioLevel(from: audioBuffer)
        logger.info("ğŸµ è¯­éŸ³è¯†åˆ«éŸ³é¢‘ç”µå¹³: \(String(format: "%.6f", audioLevel))")
        
        if audioLevel < 0.001 {
            logger.warning("âš ï¸ éŸ³é¢‘ç”µå¹³å¤ªä½ï¼Œå¯èƒ½æ˜¯é™éŸ³æ•°æ®")
        }
        
        // ä¿å­˜è½¬æ¢åçš„éŸ³é¢‘æ•°æ®ç”¨äºéªŒè¯
        saveConvertedAudioBuffer(audioBuffer)
        
        // Note: Audio data is processed through the existing Darwin notification system
        // which feeds into the Socket.IO connection established separately
        
        logger.debug("âœ… ä½¿ç”¨é‡å»ºçš„CMSampleBufferè¿›è¡ŒSocket.IOè¯­éŸ³è¯†åˆ« (ç”µå¹³: \(String(format: "%.6f", audioLevel)))")
    }
    
    private func calculateSimpleAudioLevel(from audioBuffer: AVAudioPCMBuffer) -> Double {
        guard audioBuffer.frameLength > 0 else { return 0.0 }
        
        let format = audioBuffer.format
        let frameCount = Int(audioBuffer.frameLength)
        let channels = Int(format.channelCount)
        
        var sum: Double = 0.0
        var sampleCount = 0
        
        // å¤„ç†äº¤é”™æ ¼å¼çš„éŸ³é¢‘æ•°æ®
        if let audioData = audioBuffer.audioBufferList.pointee.mBuffers.mData {
            if format.commonFormat == .pcmFormatInt16 {
                // 16ä½æ•´æ•°æ ¼å¼
                let int16Pointer = audioData.bindMemory(to: Int16.self, capacity: frameCount * channels)
                for i in 0..<(frameCount * channels) {
                    let sample = Double(int16Pointer[i]) / 32768.0
                    sum += sample * sample
                    sampleCount += 1
                }
            } else if format.commonFormat == .pcmFormatFloat32 {
                // 32ä½æµ®ç‚¹æ ¼å¼
                let floatPointer = audioData.bindMemory(to: Float.self, capacity: frameCount * channels)
                for i in 0..<(frameCount * channels) {
                    let sample = Double(floatPointer[i])
                    sum += sample * sample
                    sampleCount += 1
                }
            }
        }
        
        guard sampleCount > 0 else { return 0.0 }
        
        let rms = sqrt(sum / Double(sampleCount))
        return rms
    }
    
    private func createAudioPCMBufferFromSampleBuffer(_ sampleBuffer: CMSampleBuffer) -> AVAudioPCMBuffer? {
        guard let formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer) else {
            logger.error("âŒ æ— æ³•è·å–CMSampleBufferæ ¼å¼æè¿°")
            return nil
        }
        
        let streamDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription)
        guard let audioStreamDescription = streamDescription else {
            logger.error("âŒ æ— æ³•è·å–éŸ³é¢‘æµæè¿°")
            return nil
        }
        
        logger.info("ğŸ” è¾“å…¥éŸ³é¢‘æ ¼å¼: \(audioStreamDescription.pointee.mSampleRate)Hz, \(audioStreamDescription.pointee.mChannelsPerFrame)å£°é“, \(audioStreamDescription.pointee.mBitsPerChannel)ä½")
        
        // ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ ¼å¼è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œä¸è¿›è¡Œå¤æ‚çš„æ ¼å¼è½¬æ¢
        guard let inputAVFormat = AVAudioFormat(streamDescription: audioStreamDescription) else {
            logger.error("âŒ åˆ›å»ºè¾“å…¥éŸ³é¢‘æ ¼å¼å¤±è´¥")
            return nil
        }
        
        // è·å–è¾“å…¥æ•°æ®
        guard let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) else {
            logger.error("âŒ æ— æ³•è·å–CMSampleBufferæ•°æ®ç¼“å†²åŒº")
            return nil
        }
        
        let dataLength = CMBlockBufferGetDataLength(blockBuffer)
        let inputFrameCount = AVAudioFrameCount(dataLength / Int(audioStreamDescription.pointee.mBytesPerFrame))
        
        logger.info("ğŸ” éŸ³é¢‘æ•°æ®: é•¿åº¦=\(dataLength)å­—èŠ‚, å¸§æ•°=\(inputFrameCount)")
        
        guard inputFrameCount > 0 else {
            logger.warning("âš ï¸ éŸ³é¢‘å¸§æ•°ä¸º0ï¼Œè·³è¿‡å¤„ç†")
            return nil
        }
        
        guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: inputAVFormat, frameCapacity: inputFrameCount) else {
            logger.error("âŒ åˆ›å»ºè¾“å…¥PCMç¼“å†²åŒºå¤±è´¥")
            return nil
        }
        
        // å¤åˆ¶æ•°æ®åˆ°è¾“å…¥ç¼“å†²åŒº
        var dataPointer: UnsafeMutablePointer<Int8>?
        let result = CMBlockBufferGetDataPointer(blockBuffer, atOffset: 0, lengthAtOffsetOut: nil, totalLengthOut: nil, dataPointerOut: &dataPointer)
        
        guard result == noErr, let data = dataPointer else {
            logger.error("âŒ æ— æ³•è·å–éŸ³é¢‘æ•°æ®æŒ‡é’ˆ: \(result)")
            return nil
        }
        
        // ç›´æ¥å¤åˆ¶éŸ³é¢‘æ•°æ®
        let audioBufferList = inputBuffer.mutableAudioBufferList
        audioBufferList.pointee.mBuffers.mData?.copyMemory(from: data, byteCount: dataLength)
        audioBufferList.pointee.mBuffers.mDataByteSize = UInt32(dataLength)
        inputBuffer.frameLength = inputFrameCount
        
        logger.info("âœ… éŸ³é¢‘PCMç¼“å†²åŒºåˆ›å»ºæˆåŠŸ: \(inputFrameCount)å¸§")
        
        return inputBuffer
    }
    
    // å®Œå…¨æ¨¡ä»¿ScreenBroadcastHandlerçš„stopAudioRecordingæ–¹æ³•
    private func stopM4ARecording() {
        guard let writer = m4aAudioWriter else { return }
        
        m4aAudioWriterInput?.markAsFinished()
        
        // ä¿å­˜URLçš„å‰¯æœ¬ï¼Œé˜²æ­¢åœ¨å¼‚æ­¥å—ä¸­è¢«æ¸…ç©º
        let audioFileURL = currentM4AFileURL
        
        writer.finishWriting { [weak self] in
            if writer.status == .completed {
                self?.logger.info("âœ… éŸ³é¢‘æ–‡ä»¶å½•åˆ¶å®Œæˆ")
                if let url = audioFileURL {
                    Task { @MainActor in
                        self?.notifyM4AAudioFileCompleted(fileURL: url)
                    }
                    self?.logger.info("ğŸ“ éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: \(url.lastPathComponent)")
                }
            } else if let error = writer.error {
                self?.logger.error("âŒ éŸ³é¢‘æ–‡ä»¶å†™å…¥å¤±è´¥: \(error.localizedDescription)")
            }
            
            // æ¸…ç†å¼•ç”¨
            Task { @MainActor in
                self?.m4aAudioWriter = nil
                self?.m4aAudioWriterInput = nil
                self?.currentM4AFileURL = nil
                self?.m4aStartTime = nil  // ç¡®ä¿é‡ç½®å¼€å§‹æ—¶é—´
            }
        }
    }
    
    private func notifyM4AAudioFileCreated(fileName: String, fileURL: URL) {
        let notification: [String: Any] = [
            "event": "audio_file_created",
            "fileName": fileName,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("realtime_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            logger.info("ğŸ“¡ å·²é€šçŸ¥M4AéŸ³é¢‘æ–‡ä»¶åˆ›å»º")
        } catch {
            logger.error("âŒ å†™å…¥M4AéŸ³é¢‘åˆ›å»ºé€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func notifyM4AAudioFileCompleted(fileURL: URL) {
        let notification: [String: Any] = [
            "event": "audio_file_completed",
            "fileName": fileURL.lastPathComponent,
            "filePath": fileURL.path,
            "timestamp": Date().timeIntervalSince1970
        ]
        
        guard let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: appGroupID) else {
            return
        }
        
        let notificationURL = containerURL.appendingPathComponent("realtime_audio_notification.json")
        
        do {
            let jsonData = try JSONSerialization.data(withJSONObject: notification, options: [])
            try jsonData.write(to: notificationURL)
            
            // å‘é€Darwiné€šçŸ¥
            let darwinCenter = CFNotificationCenterGetDarwinNotifyCenter()
            let notificationName = CFNotificationName("dev.tuist2.Siri.realtimeAudioSaved" as CFString)
            CFNotificationCenterPostNotification(darwinCenter, notificationName, nil, nil, true)
            
            logger.info("ğŸ“¡ å·²é€šçŸ¥M4AéŸ³é¢‘æ–‡ä»¶å®Œæˆ")
        } catch {
            logger.error("âŒ å†™å…¥M4AéŸ³é¢‘å®Œæˆé€šçŸ¥å¤±è´¥: \(error.localizedDescription)")
        }
    }
}

